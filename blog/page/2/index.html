<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-list-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.3">
<title data-rh="true">Blog | Mokronos&#x27;s Site</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://mokronos.github.io/blog/page/2"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" property="og:title" content="Blog | Mokronos&#x27;s Site"><meta data-rh="true" name="description" content="Blog"><meta data-rh="true" property="og:description" content="Blog"><meta data-rh="true" name="docusaurus_tag" content="blog_posts_list"><meta data-rh="true" name="docsearch:docusaurus_tag" content="blog_posts_list"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://mokronos.github.io/blog/page/2"><link data-rh="true" rel="alternate" href="https://mokronos.github.io/blog/page/2" hreflang="en"><link data-rh="true" rel="alternate" href="https://mokronos.github.io/blog/page/2" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Mokronos&#39;s Site RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Mokronos&#39;s Site Atom Feed">




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous">
<script src="js/hotkey.js"></script><link rel="stylesheet" href="/assets/css/styles.12cf0569.css">
<link rel="preload" href="/assets/js/runtime~main.3350fcfb.js" as="script">
<link rel="preload" href="/assets/js/main.204eeccb.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"dark")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="My Site Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/logo.svg" alt="My Site Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">Mokronos&#x27;s Site</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/mokronos" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently dark mode)" aria-label="Switch between dark and light mode (currently dark mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search"><span aria-label="expand searchbar" role="button" class="search-icon" tabindex="0"></span><input type="search" id="search_input_react" placeholder="Loading..." aria-label="Search" class="navbar__search-input search-bar" disabled=""></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/ai">Artificial Intelligence</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/bayesian_thinking">Bayesian thinking</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/classes">classes</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/coding">Coding</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/cs50">CS50</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/daily">Daily ToDos</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/data_preparation">Data Preparation and Feature Engineering</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/deep_learning">Training Guide</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/docker">Docker</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/electric_vehicles">Electric vehicles</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/etf_investing">Investing</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/fakenews">Fake news</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/habits">Habits</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/how_to_code">How to code</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/hri">Human Robot Interaction</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/logic">Logic</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/machine_learning">Machine Learning</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/neural_network">Neural network</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/notes">notes</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/pytorch">PyTorch</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/search">Search</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/sort">Sort</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/tidy_data">Tidy data</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/unknown">Knowing the unknown</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="description" content="!!! Es wurden nur 1-2 verschiedene Quellen angegeben, allerdings sind die meisten Aussagen nicht wirklich kontrovers. Die meisten Dinge lassen sich schnell auf Investopedia oder anderen Seiten finden. Ich war nur zu faul alles anzugeben."><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/etf_investing">Investing</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2023-11-07T12:35:59.000Z" itemprop="datePublished">November 7, 2023</time> · <!-- -->4 min read</div></header><div class="markdown" itemprop="articleBody"><p><strong>!!! Es wurden nur 1-2 verschiedene Quellen angegeben, allerdings sind die meisten Aussagen nicht wirklich kontrovers. Die meisten Dinge lassen sich schnell auf Investopedia oder anderen Seiten finden. Ich war nur zu faul alles anzugeben.</strong></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="timeframe">Timeframe<a href="#timeframe" class="hash-link" aria-label="Direct link to Timeframe" title="Direct link to Timeframe">​</a></h2><p>~15 years</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="etf">ETF<a href="#etf" class="hash-link" aria-label="Direct link to ETF" title="Direct link to ETF">​</a></h2><p>ETFs sind wie Fonds, kleine Anteile von Unternehmen in &quot;Packet&quot;. Kann dadurch über verschiedene Branchen und über die ganze Welt geographisch gestreut werden. Daraus folgt, dass nicht mehr in bestimmte Unternehmen, sondern in den <strong>gesamten Markt</strong> investiert wird.</p><p>Der Unterschied von ETFs im Vergleich zu Fonds ist das Management. Fonds werden von Angestellten von Banken gemanaged und <strong>aktiv Aktien hinzugefügt oder verkauft</strong>. Dies ist mit <strong>Personalkosten</strong> verbunden. ETFs werden <strong>automatisch gemanaged</strong> und versuchen so genau wie möglich einem <strong>Index zu folgen</strong>. Diese Indizes werden auf Basis von bestimmten Regeln aufgestellt. Mcsi world setzt sich beispielsweise aus den top (70%) Unternehmen mit den höchsten Marktkapitalisierung aus bestimmten Ländern zusammen. Die Kostenunterschiede zwischen ETFs und aktiven Fonds beträgt meist <strong>~1%</strong>.</p><p>Diese Differenz muss durch die Experten der Banken, welche diese Fonds managen, <strong>zusätzlich zu der Rendite des ETFs</strong> erreicht werden, sodass der Fond ein besseres Investment als der ETF ist. Dies wird meist nicht erreicht bzw. ist wieder ein Glücksspiel in sich selbst, da der &quot;richtige&quot; Fond herausgesucht werden muss und oft sind diese nicht wirklich transparent. Zudem ist es so gut wie <strong>unmöglich den Markt zu verstehen</strong> und falls Experten dies täten, würden sie vermutlich keine Fonds managen sondern selbst traden und nicht mehr arbeiten müssen.</p><p>Die Annahme, dass der <strong>Markt immer weiter steigt</strong>, ist warscheinlich korrekt. Menschen wollen schon immer mehr Dinge und ein &quot;besseres&quot; Leben. Die Industrieländer sind vielen Teilen der Welt vorraus. Auch wenn die Industrieländer sich entscheiden sollten nicht mehr so viel zu produzieren (evtl. auf Grund des Klimawandels) wird vermutlich immer noch der Rest der Welt nachziehen und Güter benötigen. Zudem wird dies vermutlich nicht in den nächsten ~30 Jahren passieren, wenn überhaupt.</p><p><a href="https://ystat.org/" target="_blank" rel="noopener noreferrer">Mcsi world über 15 jahre</a> (seite down <!-- -->→<!-- --> <a href="https://backtest.curvo.eu/portfolio/msci-world--NoIgsgygwgkgBAdQPYCcA2ATEAaYoAyAqgIwDsAHMQKwAsxZAnDsQLptA" target="_blank" rel="noopener noreferrer">ähnlich</a>) median rendite 8.78%, <strong>schlechteste 3.16% pro jahr</strong> (abhängig davon wann gekauft/verkauft wird). Das wichtigste ist, dass die varianz mit längeren Investmentperioden sinkt und nach 10-15 Jahren die Chance für positive Rendite über 90% liegt und nach 20 Jahren gar bei 100% (auf der <a href="https://backtest.curvo.eu/portfolio/msci-world--NoIgsgygwgkgBAdQPYCcA2ATEAaYoAyAqgIwDsAHMQKwAsxZAnDsQLptA" target="_blank" rel="noopener noreferrer">Seite</a> zu Minimum investment horizon und Compute clicken). Natürlich sind dies historische Daten und garantieren nichts für die Zukunft, jedoch sind sie trotzdem gute Anhaltspunkte, da der gesamte Markt sich in Zukunft vermutlich auf lange Sicht ähnlich verhalten wird.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="broker">Broker<a href="#broker" class="hash-link" aria-label="Direct link to Broker" title="Direct link to Broker">​</a></h2><ul><li>Hohe Sicherheit (haben eigentlich alle hier in Deutschland + deutsche Einlagensicherung bis 100.000€)</li><li>keine Verwahrungsgebüren (ist bei den meisten auch so)</li><li>geringe Kosten für einmaligen Kauf (gut wenn pauschal, schlecht wenn %)</li></ul><p>Es ist schwer Beispiele zu nennen, da die Konditionen der Broker sich andauernd ändern. Generell bieten jedoch die &quot;alten&quot; Banken schlechtere Konditionen. Ein weiterer wichtiger Faktor ist oft, ob die Broker eine gute App und Website anbieten, da viele Banken noch sehr hinterher sind. Am besten kurz immer YouTube Videos angucken, um die Apps in Aktion zu sehen. Man könnte allerdings auch argumentieren, dass es besser ist keine oder keine gute App zu haben, da dies bei vielen Leuten zu Impuls Käufen oder Verkäufen führen kann, da es so einfach ist. </p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="nachhaltigkeit">Nachhaltigkeit<a href="#nachhaltigkeit" class="hash-link" aria-label="Direct link to Nachhaltigkeit" title="Direct link to Nachhaltigkeit">​</a></h2><ul><li><strong>ESG</strong>: Ziel ist immer noch Performance. Es werden Unternehmen ausgeschlossen die durch bestimmte umweltschädlichen, sozialen oder ihr Regime negativ auffallen, und dadurch auch eventuell schlechtere Performance erreichen (so weit ich weiß wurde aber noch keine klare Korrelation mit schlechterer Performance festgestellt).</li><li><strong>SRI</strong>: strenger als ESG, suchen aktiv Unternehmen nach ethischen Richtlinien herraus, verwenden allerdings auch teilweise das ESG-Rating zur auswahl, nur die Grenze ist strenger.</li></ul><p>Gute Videos dafür: <a href="https://www.youtube.com/watch?v=6kIzjD_seLI" target="_blank" rel="noopener noreferrer">[1]</a><a href="https://www.youtube.com/watch?v=VeBHRURmh1U" target="_blank" rel="noopener noreferrer">[2]</a> oder in <a href="https://www.finanzfluss.de/geldanlage/nachhaltige-etfs/" target="_blank" rel="noopener noreferrer">Text Form</a>.</p><p>Oder einfach <a href="https://www.investopedia.com/financial-advisor/esg-sri-impact-investing-explaining-difference-clients/" target="_blank" rel="noopener noreferrer">Investopedia</a>.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="7030">70/30<a href="#7030" class="hash-link" aria-label="Direct link to 70/30" title="Direct link to 70/30">​</a></h2><p>Mit dem MSCI World werden nur die Industrieländer abgedeckt. Um noch breiter zu diversifizieren ist es sinnvoll ca. 30% des Portfolios in beispielsweise den MSCI World Emerging Markets (EM), welcher viele Schwellenländer noch mit abdeckt. Alternativ ist es noch möglich etwas Europa dazuzunehmen, wenn die Übergewichtung der USA im MSCI World nicht gefällt.</p><p><a href="https://www.finanzfluss.de/etf-handbuch/etf-portfolio/" target="_blank" rel="noopener noreferrer">Hier</a> noch ein paar andere Porfolioarten zum Vergleich.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="mögliche-etfs">Mögliche ETFs<a href="#mögliche-etfs" class="hash-link" aria-label="Direct link to Mögliche ETFs" title="Direct link to Mögliche ETFs">​</a></h2><p><strong>MSCI World SRI (70%):</strong></p><ul><li>Amundi Index MSCI World SRI UCITS ETF DR (WKN: A2JSDA ISIN: LU1861134382)</li></ul><p><strong>MSCI World Emerging Markets SRI (30%):</strong></p><ul><li>iShares MSCI EM SRI UCITS ETF (WKN: A2AFCZ ISIN: IE00BYVJRP78)</li><li>Amundi Index MSCI Emerging Markets SRI UCITS DR ETF (C) (WKN: A2JSDD ISIN: LU1861138961)</li></ul><p>Zum finden ist <a href="https://justetf.com/en/find-etf.html" target="_blank" rel="noopener noreferrer">justETF</a> gut. Einfach einen Index auswählen und nach Kriterien filtern. Hier noch eine <a href="https://www.finanzfluss.de/etf-handbuch/etf-auswahl-kriterien/" target="_blank" rel="noopener noreferrer">Seite</a> für Tips auf welche Dinge bei der Auswahl zu achten ist.</p></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="description" content="Google bias"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/fakenews">Fake news</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2023-11-07T12:35:59.000Z" itemprop="datePublished">November 7, 2023</time> · <!-- -->11 min read</div></header><div class="markdown" itemprop="articleBody"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="google-bias">Google bias<a href="#google-bias" class="hash-link" aria-label="Direct link to Google bias" title="Direct link to Google bias">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="the-enduring-anti-black-racism-of-google-search">The Enduring Anti-Black Racism of Google Search<a href="#the-enduring-anti-black-racism-of-google-search" class="hash-link" aria-label="Direct link to The Enduring Anti-Black Racism of Google Search" title="Direct link to The Enduring Anti-Black Racism of Google Search">​</a></h3><p>link: <a href="https://onezero.medium.com/the-enduring-anti-black-racism-of-google-search-d024924bff77" target="_blank" rel="noopener noreferrer">https://onezero.medium.com/the-enduring-anti-black-racism-of-google-search-d024924bff77</a></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="bad-article">Bad article<a href="#bad-article" class="hash-link" aria-label="Direct link to Bad article" title="Direct link to Bad article">​</a></h4><p>A search engines job should be to get users the information they want. It feels like the baseline for zero bias is what the internet offers. So when the whole internet is nine images of an apple and one image of a banana, it would be good to expect an apple more often when searching for &quot;fruit&quot;.</p><p>So when there are the tags for &quot;black girls&quot; are overwhelmingly found on porn sites it should be to no surprise that google shows those when searching for &quot;black girls&quot;.</p><p>It feels like, if google would not intervene in the &quot;societal&quot; distribution of the search results, we would have more racist search results, as one can see at the exact example in the article. So to blame google instead of the culture of the people is wrong. So google actually does bias the results, however one could argue that that&#x27;s a good thing. The author shouldn&#x27;t blame google for the few times reality was shining through, but thank google for adjusting some bad baselines in society.</p><p>It might be helpful to get some diversification in the workplace to help these things, however those need not necessarily be engineers, as the manual adjustments shouldn&#x27;t have anything to do with engineering, rather with social studies or ethics.</p><p>The whole thing is hard to evaluate since we don&#x27;t exactly know how google selects the search results, however the alternatives are hard to imagine. Is the implication of the article that google engineers went ahead and weighted porn websites higher when searching for &quot;black girls&quot; instead of &quot;white girls&quot;? What would be the motive behind that?</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="some-weird-quotes">Some weird quotes<a href="#some-weird-quotes" class="hash-link" aria-label="Direct link to Some weird quotes" title="Direct link to Some weird quotes">​</a></h4><blockquote><p>Pornography is a specific type of representation that denotes male power, female powerlessness, and sexual violence.</p></blockquote><p>This feels wrong.</p><blockquote><p>Porn on the internet is an expansion of neoliberal capitalist interests. The web itself has opened up new centers of profit and pushed the boundaries of consumption. Never before have there been so many points for the transmission and consumption of these representations of Black women’s bodies, largely trafficked outside the control and benefit of Black women and girls themselves.</p></blockquote><ul><li>Why is an expansion of capitalist interests bad?</li><li>Isn&#x27;t that good for a lot of people?</li><li>What does neoliberalism have to do with it? </li><li>The assumption that the consumption is &quot;largely&quot; outside the benefit of black woman needs to be justified.<ul><li>black woman that do porn do benefit? If they are exploited there is another question. (more fame, money, like music labels)</li><li>why only for black woman? And what about men in porn? (earn way less than woman)</li></ul></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="gunn-and-lynch-googling-2019">Gunn and Lynch Googling (2019)<a href="#gunn-and-lynch-googling-2019" class="hash-link" aria-label="Direct link to Gunn and Lynch Googling (2019)" title="Direct link to Gunn and Lynch Googling (2019)">​</a></h3><p>Chapter 4, page 41-51</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="risks-of-googling">Risks of googling<a href="#risks-of-googling" class="hash-link" aria-label="Direct link to Risks of googling" title="Direct link to Risks of googling">​</a></h4><ul><li>anonymity, no accountability of face-to-face information. Not a google issue, internet issue.</li><li>no way to make sure that information is good, same issue in real life, but internet gives broader view quickly. One needs to check multiple sources, integrate in world view and check for inconsistencies.</li><li>reductionism<ul><li>reductionism: trust is earned not assumed</li><li>anti-reductionism: just trust others, assuming they have similar thinking faculties</li></ul></li><li>google searches lead to most wanted results, assumed to be right, because many others found it to be right</li><li>trust if author is trusted, problematic, but not unique to internet<ul><li>collective of authors (wikis)</li><li>some are automated (currency exchange)</li></ul></li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="selecting-sources">selecting sources<a href="#selecting-sources" class="hash-link" aria-label="Direct link to selecting sources" title="Direct link to selecting sources">​</a></h4><ul><li>more reliable information is shared more, higher in ranking, good</li><li>sometimes information of influencers does have popularity-because-popular, bad</li><li>there are often experts on both sides, use institutional markers</li><li>use of likes, upvotes, titles (must not necessarily be a good marker)</li><li>advertising can be misleading</li><li>Wikipedia can be good as everything needs source from expert, but can be outdated due to lag of checkups</li><li>not all people are able to select &quot;good&quot; sources, most just accept everything google gives them first.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="google-in-brain">Google in Brain<a href="#google-in-brain" class="hash-link" aria-label="Direct link to Google in Brain" title="Direct link to Google in Brain">​</a></h4><ul><li>extend brain with chip, access fast amounts of information at speed of thought</li><li>accelerates effects/issues above</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="epistemic-agency">Epistemic Agency<a href="#epistemic-agency" class="hash-link" aria-label="Direct link to Epistemic Agency" title="Direct link to Epistemic Agency">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="tugend">Tugend<a href="#tugend" class="hash-link" aria-label="Direct link to Tugend" title="Direct link to Tugend">​</a></h3><p>Verantwortung
Anonymity</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="on-trusting-wikipedia">On trusting Wikipedia<a href="#on-trusting-wikipedia" class="hash-link" aria-label="Direct link to On trusting Wikipedia" title="Direct link to On trusting Wikipedia">​</a></h2><ul><li>generally trusted</li><li>not always correct</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="wikipedia-and-the-epistemology-of-testimony-by-deborah-perron-tollefsen">Wikipedia and the epistemology of testimony (by deborah perron tollefsen)<a href="#wikipedia-and-the-epistemology-of-testimony-by-deborah-perron-tollefsen" class="hash-link" aria-label="Direct link to Wikipedia and the epistemology of testimony (by deborah perron tollefsen)" title="Direct link to Wikipedia and the epistemology of testimony (by deborah perron tollefsen)">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="introduction">Introduction<a href="#introduction" class="hash-link" aria-label="Direct link to Introduction" title="Direct link to Introduction">​</a></h3><ul><li>studies mainly focus on individual testimony</li><li>group testimony can&#x27;t always be understood as just a summation of individual testimony</li><li>the group itself testifies</li><li>example Wikipedia</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="questions">Questions<a href="#questions" class="hash-link" aria-label="Direct link to Questions" title="Direct link to Questions">​</a></h3><ul><li>Is Wikipedia a source of testimony?</li><li>What is the nature of that source?<ul><li>the individuals that make entries</li><li>a subset of individuals</li><li>the entity Wikipedia itself</li></ul></li><li>How can we asses the trustworthiness of Wikipedia as such an unusual epistemic source?</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="are-the-statements-on-wikipedia-testimony">Are the statements on Wikipedia testimony?<a href="#are-the-statements-on-wikipedia-testimony" class="hash-link" aria-label="Direct link to Are the statements on Wikipedia testimony?" title="Direct link to Are the statements on Wikipedia testimony?">​</a></h3><h4 class="anchor anchorWithStickyNavbar_LWe7" id="define-testimony">define testimony<a href="#define-testimony" class="hash-link" aria-label="Direct link to define testimony" title="Direct link to define testimony">​</a></h4><ul><li>conservative (Coady 1992)<ul><li>speakers intention to present evidence on a specific matter in the interest of the audience</li></ul></li><li>liberal (E. Fricker 1995, Sosa 1991)<ul><li>&quot;tellings in general&quot; with no restriction on the domain</li></ul></li><li>Jennifer Lackey (2006)<ul><li>&quot;S testifies that p by making an act of communication a if and only if (in part) in virtue of a’s communicable content, (1) S reasonably intends to convey the information that p, or (2) a is reasonably taken as conveying the information that p.&quot;</li><li>so it is a testimony if the speaker intends to convey information or if the audience takes it as such</li></ul></li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="wikipedia-as-testimony">Wikipedia as testimony<a href="#wikipedia-as-testimony" class="hash-link" aria-label="Direct link to Wikipedia as testimony" title="Direct link to Wikipedia as testimony">​</a></h4><p>All would obviously include Wikipedia as testimony.</p><p>Assumption: People are trolling, writing false information for fun.</p><p>Some definitions of testimony might be broken. Lackey&#x27;s definition would still include Wikipedia as testimony, as people who read Wikipedia still assume it to be testimony.</p><p>Wray: not all entries are testimony, some are jokes, so nothing is testimony.</p><p>Doesn&#x27;t mean nothing is testimony.</p><p>Testimony is not only what one believes, otherwise there would be no false testimony.</p><p>Moran(2006), Assurance view: Testimony comes with assurance that statement is true. Testifiers have responsibility to be truthful. They are aware that they might be questioned and need to explain themselves if the statement is false.</p><p>The same is true on Wikipedia. People can change information, but they know that they can then be called to question by other people that can discuss these changes and change them again.</p><p>Even if a troll is sometimes hard to track down and question, the information still is taken to come with assurances.</p><p>So none of the definitions of testimony would exclude Wikipedia as testimony.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="group-testimony">Group testimony<a href="#group-testimony" class="hash-link" aria-label="Direct link to Group testimony" title="Direct link to Group testimony">​</a></h3><p>New Question: Is the source the person that writes the entry or the entity Wikipedia?</p><p>When group decides something, it doesn&#x27;t necessarily follow that all or most of the group, would testify the similarly.</p><p><strong>Example:</strong></p><p>NAS needed to make statement on long term genetic hazards of radiation exposure. It was a difficult decision, but needed to be made to protect the public from other, more harmful misinformation.  Some scientists even refused to sign it, because they thought it was indeterminable. In the end they all signed.</p><p>For a group G, speaker S, and utterance x, G utters x if and only if:</p><ol><li>There exists a group (G), this group has an illocutionary intention, and x conveys that illocutionary intention.</li><li>S believes that he or she knows the illocutionary intention of G and that x conveys this illocutionary intention.</li><li>G does not object to S uttering x on its behalf and if G intends for any specific individual(s) to utter x, it intends for S to utter x. S believes that he or she knows this.</li><li>2 and 3 are the reasons S utters x.</li></ol><p>Need to add 5th condition.</p><ol start="5"><li>S utters G in the proper social and normative context.</li></ol><p>This is important, as the NAS group would probably not have signed the statements, if it wasn&#x27;t necessary to keep public trust and safety.</p><p>So group testimony (group speech act with conveyed information):</p><p>Group G testifies that p by making an act of communication a if and only if:</p><ol><li>(in part) in virtue of a’s communicable content G reasonably intends to convey the information that p.</li><li>The information that p is conveyed by either (i) a spokesperson S or (ii) a written document.</li><li>If (i), G does not object to S’s uttering p on its behalf and if G intends for any specific individual(s) to utter p, it intends for S to utter p and S believes that he or she knows this.</li><li>If (i), S utters p for the reasons in 3.</li><li>If (ii), G does not object to the way in which p is conveyed in writing.</li><li>G conveys the information that p in the right social and normative context.</li><li>In conveying the information that p in the right social and normative context, G is taken to have given its assurance that p is true.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="wikipedia-entries-as-group-testimony">Wikipedia entries as group testimony<a href="#wikipedia-entries-as-group-testimony" class="hash-link" aria-label="Direct link to Wikipedia entries as group testimony" title="Direct link to Wikipedia entries as group testimony">​</a></h3><p>Traits shared by groups (research teams, governments or corporations)</p><ul><li>share certain goals<ul><li>clear goals on Wikipedia: natural, balanced, verifiable knowledge to all for free</li><li>contributors have largely those goals, would be hard to explain otherwise</li></ul></li><li>are aware that they share these goals<ul><li>&quot;Wikipedia community&quot;, &quot;Wikipedians&quot; are names used, Wikipedia conferences exist</li><li>there are pages on Wikipedia that explain itself, so its self reflective</li></ul></li><li>group decision making process with specific rules</li><li>group members have special rights and obligations</li></ul><p>Articles are testimony of Wikipedia once they have been discussed at length and have been approved by the community, they become featured or good articles.</p><p>Until then, they are either individual or group testimony.</p><p>The trustworthiness of Wikipedia needs to be monitored in those early stages, while &quot;steadying ones mind&quot;, almost like a child.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="trustworthiness-of-wikipedia">Trustworthiness of Wikipedia<a href="#trustworthiness-of-wikipedia" class="hash-link" aria-label="Direct link to Trustworthiness of Wikipedia" title="Direct link to Trustworthiness of Wikipedia">​</a></h3><p>Anti-reductionism: Trust others, assuming they have similar thinking faculties. (normal conversation)
Reductionism: Trust, if there are positive reasons, that the other person is sincere/reliable.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="anti-reductionism">Anti-reductionism<a href="#anti-reductionism" class="hash-link" aria-label="Direct link to Anti-reductionism" title="Direct link to Anti-reductionism">​</a></h4><p>Normal conversation with one person is on topics with not expertise, so anti-reductionism is fine. Generally groups have some kind of specific expertise (governmental, scientific, legal). With Wikipedia its different, because it speaks on a wide range of interests (more like a person on the street).</p><p>However the standard trust assumption of anti-reductionism may fail with Wikipedia, if it&#x27;s treated as a child/unstable.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="reductionism">Reductionism<a href="#reductionism" class="hash-link" aria-label="Direct link to Reductionism" title="Direct link to Reductionism">​</a></h4><h5 class="anchor anchorWithStickyNavbar_LWe7" id="scrutinize-the-speakerwikipedia">Scrutinize the speaker/Wikipedia<a href="#scrutinize-the-speakerwikipedia" class="hash-link" aria-label="Direct link to Scrutinize the speaker/Wikipedia" title="Direct link to Scrutinize the speaker/Wikipedia">​</a></h5><ul><li>Sum of individuals:<ul><li>check if some or all of the contributors are reliable</li><li>often short track record of contributors, hard to evaluate</li><li>mature articles could be closer to the truth than the individual entries of the contributors through the process of discussion and approval</li><li>might tell us nothing about the trustworthiness of Wikipedia</li></ul></li><li>Systematic cues:<ul><li>Programs can figure out anomalies (quick changes without discussion after long stable period, information that doesn&#x27;t fit into the style of the article)</li><li>quick correction of spelling/grammatical errors might hide red flags for the content of the entry from the reader; one can still check the history (most probably wont)</li><li>trust the system not the individuals</li></ul></li></ul><h5 class="anchor anchorWithStickyNavbar_LWe7" id="scrutinize-the-content">Scrutinize the content<a href="#scrutinize-the-content" class="hash-link" aria-label="Direct link to Scrutinize the content" title="Direct link to Scrutinize the content">​</a></h5><p>Verify with own background knowledge. Integrate in world view and check for inconsistencies.
A UFO landed on the school roof (unlikely because of prior believe)
Trust the process and the reports, not the individuals.
There are incentives for groups to tell the truth(scientific groups, some corporations)</p><p>If group testimony is wildly at odds with your own knowledge, one has no reason to trust it.</p><p><strong>Wikipedia</strong>:</p><ul><li>incentives are in the structure of the system</li><li>entries will be checked against background knowledge</li><li>by challenging Wikipedia, its reliability increases through new policies and procedures</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h3><ul><li>Wikipedia involves a mix of individual, group and Wikipedia testimony</li><li>Can&#x27;t trust Wikipedia by default, yet (still a child)</li><li>Will get better once it matures, and doesn&#x27;t need to be constantly monitored</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="discussion-questions">Discussion Questions<a href="#discussion-questions" class="hash-link" aria-label="Direct link to Discussion Questions" title="Direct link to Discussion Questions">​</a></h3><h2 class="anchor anchorWithStickyNavbar_LWe7" id="fake-news-and-partisan-epistemology">Fake News and Partisan Epistemology<a href="#fake-news-and-partisan-epistemology" class="hash-link" aria-label="Direct link to Fake News and Partisan Epistemology" title="Direct link to Fake News and Partisan Epistemology">​</a></h2><h2 class="anchor anchorWithStickyNavbar_LWe7" id="context-collapse">Context Collapse<a href="#context-collapse" class="hash-link" aria-label="Direct link to Context Collapse" title="Direct link to Context Collapse">​</a></h2><h2 class="anchor anchorWithStickyNavbar_LWe7" id="hopeful-trust">Hopeful Trust<a href="#hopeful-trust" class="hash-link" aria-label="Direct link to Hopeful Trust" title="Direct link to Hopeful Trust">​</a></h2><p>Pendulum swung too far:</p><ul><li><p>Tweets with misinformation are taken as truth because of labels of person (trans, black); Trust gets abused by speaker. It is assumed people trust the person just because of the label. Examples disproportionally damage trust in truthful speakers.</p><ul><li>trans person claiming periods <a href="https://twitter.com/poisonaivy69/status/1507362480158355508" target="_blank" rel="noopener noreferrer">https://twitter.com/poisonaivy69/status/1507362480158355508</a></li><li>jakob blake misinfo</li></ul></li><li><p>vulnerability invites trust, but only sometimes. On social media, one needs to take the average to see effect</p><ul><li>trump voters meet trans people in real life</li></ul></li></ul></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="description" content="Don&#x27;t multitask"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/habits">Habits</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2023-11-07T12:35:59.000Z" itemprop="datePublished">November 7, 2023</time> · <!-- -->4 min read</div></header><div class="markdown" itemprop="articleBody"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="dont-multitask">Don&#x27;t multitask<a href="#dont-multitask" class="hash-link" aria-label="Direct link to Don&#x27;t multitask" title="Direct link to Don&#x27;t multitask">​</a></h2><p>It just doesn&#x27;t work, you think it works, but it doesn&#x27;t. You&#x27;re just switching between tasks, and you&#x27;re not doing any of them well. Or you are doing one with 80% of your brain and the other 20%, so the one with 20% isn&#x27;t even enjoyed, so why not do it with 100% of your brain in your free time.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="social-media-entertainment">Social media/ entertainment<a href="#social-media-entertainment" class="hash-link" aria-label="Direct link to Social media/ entertainment" title="Direct link to Social media/ entertainment">​</a></h2><p>Only watch them while eating or in free time. Then you can watch the highlights of the day, and not needing to search for cool stuff while you should be working or doing other things.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="sleep">Sleep<a href="#sleep" class="hash-link" aria-label="Direct link to Sleep" title="Direct link to Sleep">​</a></h2><p>Keep sleep schedule, wake up early, try 8h.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="exercise">Exercise<a href="#exercise" class="hash-link" aria-label="Direct link to Exercise" title="Direct link to Exercise">​</a></h2><p>Mix running and at home exercise, depending on weather and state of mind. Don&#x27;t miss two days in a row. Its easy to lose the habit then.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="eat-healthy">Eat healthy<a href="#eat-healthy" class="hash-link" aria-label="Direct link to Eat healthy" title="Direct link to Eat healthy">​</a></h2><p>Just count your calories, and eat filling foods. Mix it up, change some vegetables, proteins and spices in the recipes, mix up rice with noodles or lentils.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="work">Work<a href="#work" class="hash-link" aria-label="Direct link to Work" title="Direct link to Work">​</a></h2><p>Eliminate distractions. Block websites like YouTube, Twitch, Twitter, etc. Work for selected periods of time, but focused.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="do-your-bed">Do your bed<a href="#do-your-bed" class="hash-link" aria-label="Direct link to Do your bed" title="Direct link to Do your bed">​</a></h2><p>Yeah, sounds like the meme daddy Peterson tells you, but it helps, its a small task that&#x27;s easy to accomplish, and it makes you feel good. It&#x27;s a good habit to start with.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="shower">Shower<a href="#shower" class="hash-link" aria-label="Direct link to Shower" title="Direct link to Shower">​</a></h2><p>Just shower early in morning, makes you feel fresh and ready for the day. And wear clothes that you would wear in public or work. Makes you feel ready.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="walks">Walks<a href="#walks" class="hash-link" aria-label="Direct link to Walks" title="Direct link to Walks">​</a></h2><p>Take a walk without any distractions. No phone, just enjoy the nature. It&#x27;s good for your mind and body. It may sound stupid, but it will make you feel better.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="todo-list">ToDo list<a href="#todo-list" class="hash-link" aria-label="Direct link to ToDo list" title="Direct link to ToDo list">​</a></h2><p>Don&#x27;t put too many tasks on there, and make them as small as possible. That way, you can actually achieve them, and don&#x27;t feel bad because you only managed to complete half of them by the end of the day.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="free-time">Free time<a href="#free-time" class="hash-link" aria-label="Direct link to Free time" title="Direct link to Free time">​</a></h2><p>You need some dedicated time to do whatever you want. If not, it will creep into the time you should be working.</p><p>Good time to socialize. Meeting friends helps.</p><h1>Default Schedule</h1><table><thead><tr><th>Time</th><th align="center">Task</th></tr></thead><tbody><tr><td>7:00</td><td align="center">Wake up, Shower, Get ready</td></tr><tr><td>7:30</td><td align="center">Go for a walk</td></tr><tr><td>7:50</td><td align="center">Breakfast</td></tr><tr><td>8:15</td><td align="center">Schedule my day</td></tr><tr><td>8:30</td><td align="center">Work</td></tr><tr><td>12:00</td><td align="center">Do exercise</td></tr><tr><td>12:30</td><td align="center">Lunch</td></tr><tr><td>13:30</td><td align="center">Work</td></tr><tr><td>19:00</td><td align="center">Dinner</td></tr><tr><td>19:30</td><td align="center">Do whatever I want</td></tr><tr><td>22:30</td><td align="center">Sleep</td></tr></tbody></table><h1>Alternative Schedule</h1><table><thead><tr><th>Time</th><th align="center">Task</th></tr></thead><tbody><tr><td>7:00</td><td align="center">Wake up, Shower, Get ready</td></tr><tr><td>7:30</td><td align="center">Go for a walk</td></tr><tr><td>7:50</td><td align="center">Breakfast</td></tr><tr><td>8:15</td><td align="center">Schedule my day</td></tr><tr><td>8:30</td><td align="center">Work</td></tr><tr><td>12:00</td><td align="center">Do exercise</td></tr><tr><td>12:30</td><td align="center">Lunch</td></tr><tr><td>13:30</td><td align="center">Work</td></tr><tr><td>17:00</td><td align="center">Do whatever I want</td></tr><tr><td>22:30</td><td align="center">Sleep</td></tr></tbody></table><h1>Rules</h1><ol><li>Only use social media during food slots</li><li>Socialize only after 16:00, if work is done.</li><li>Can take two days off each week, if i feel like it.</li><li>If I feel healthy and motivated, I can follow the routine every single day, without off days.</li></ol><h1>Food Plan (Weekly):</h1><table><thead><tr><th>Day</th><th align="center">Meal</th></tr></thead><tbody><tr><td>Monday</td><td align="center">Tofu, zucchini, bell pepper, soy sauce, ginger stir fry with white rice</td></tr><tr><td>Tuesday</td><td align="center">Lentil soup with a side of whole grain crackers/bread</td></tr><tr><td>Wednesday</td><td align="center">Quinoa bowl with roasted mushrooms and brokkoli with lemon juice</td></tr><tr><td>Thursday</td><td align="center">Tofu, zucchini, bell pepper, soy sauce, ginger stir fry with white rice</td></tr><tr><td>Friday</td><td align="center">Spinach and tomato omelet with whole grain toast</td></tr><tr><td>Saturday</td><td align="center">Wrap with hummus, roasted eggplant and arugula</td></tr><tr><td>Sunday</td><td align="center">Tofu, zucchini, bell pepper, soy sauce, ginger stir fry with white rice</td></tr></tbody></table><p>Every day: Spices (pepper, salt, thyme, rosemary, garlic powder)</p><h1>Shopping List (Wednesday):</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="fresh">Fresh:<a href="#fresh" class="hash-link" aria-label="Direct link to Fresh:" title="Direct link to Fresh:">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="vegetablesfruits">Vegetables/Fruits:<a href="#vegetablesfruits" class="hash-link" aria-label="Direct link to Vegetables/Fruits:" title="Direct link to Vegetables/Fruits:">​</a></h3><ul><li>3x zucchini</li><li>3x bell pepper</li><li>3x mushrooms</li><li>1x eggplant</li><li>tomatoes</li><li>brokkoli</li><li>fresh spinach</li><li>arugula</li><li>3x ginger</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="other">Other:<a href="#other" class="hash-link" aria-label="Direct link to Other:" title="Direct link to Other:">​</a></h3><ul><li>3x tofu</li><li>hummus</li><li>4x eggs</li><li>wraps</li><li>cheese</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="bulk">Bulk:<a href="#bulk" class="hash-link" aria-label="Direct link to Bulk:" title="Direct link to Bulk:">​</a></h2><ul><li>soy sauce</li><li>quinoa</li><li>lentils</li><li>white rice</li><li>whole grain crackers/bread</li><li>hummus</li><li>lemon juice (oil)</li><li>spices (pepper, salt, thyme, rosemary, garlic powder)</li><li>vegetable broth powder</li></ul></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="description" content="Notes taken from &quot;What is Programming&quot; by George Hotz."><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/how_to_code">How to code</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2023-11-07T12:35:59.000Z" itemprop="datePublished">November 7, 2023</time> · <!-- -->6 min read</div></header><div class="markdown" itemprop="articleBody"><p>Notes taken from <a href="https://www.youtube.com/watch?v=N2bXEUSAiTI" target="_blank" rel="noopener noreferrer">&quot;What is Programming&quot;</a> by George Hotz.</p><h1>Programs</h1><p>Input &gt; Program &gt; Output</p><p>Seems more like functional programming.</p><p>C is like assembly with some syntactic sugar.</p><p>Language spectra:</p><ul><li>Ease of use: C --&gt; Python</li><li>Functional: C --&gt; Haskell</li></ul><p>C --&gt; C++ was functional --&gt; object oriented.</p><p>Didn&#x27;t really improve programmer productivity.</p><p>Garbage collection improved programmer productivity.</p><h1>Computer</h1><ul><li>Processor (CPU) --&gt; Stream of instructions</li><li>Memory (RAM) --&gt; Instructions + data</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="hello-world-example">Hello World Example<a href="#hello-world-example" class="hash-link" aria-label="Direct link to Hello World Example" title="Direct link to Hello World Example">​</a></h2><div class="language-c codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-c codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">int</span><span class="token plain"> </span><span class="token function" style="color:rgb(80, 250, 123)">main</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token function" style="color:rgb(80, 250, 123)">printf</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;hello world\n&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Compile with <code>gcc main.c</code> and run with <code>./a.out</code>.
The compilation will give some errors but it will still run.
And you can check out the instructions and memory with <code>objdump -D a.out</code>.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="program">Program<a href="#program" class="hash-link" aria-label="Direct link to Program" title="Direct link to Program">​</a></h2><ul><li>.text: Instructions</li><li>bss: Static data</li><li>stack: local variables (control flow)</li><li>heap: malloc</li></ul><div class="language-c codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-c codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">void</span><span class="token plain"> </span><span class="token function" style="color:rgb(80, 250, 123)">a</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">int</span><span class="token plain"> variable_on_the_stack</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    \\ </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">return</span><span class="token plain"> by popping off the stack</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">void</span><span class="token plain"> </span><span class="token function" style="color:rgb(80, 250, 123)">b</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">int</span><span class="token plain"> </span><span class="token function" style="color:rgb(80, 250, 123)">main</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token function" style="color:rgb(80, 250, 123)">a</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"> </span><span class="token comment" style="color:rgb(98, 114, 164)">// pushes the return location of the function onto the stack</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token function" style="color:rgb(80, 250, 123)">b</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>So in this example, the computer starts by executing the main function.
It then enters the a function and pushes the return location of the function onto the stack.
The a function does whatever it does, in our example, create a variable on the stack.
When the function ends, the variable gets popped off, so it does not exist anymore.
Then the return location gets popped off to know where to return to, in our case the main function after a.</p><p>This is &quot;real&quot; programming.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="programming-for-work">Programming for work<a href="#programming-for-work" class="hash-link" aria-label="Direct link to Programming for work" title="Direct link to Programming for work">​</a></h2><p>What does a software &quot;engineer&quot; do?</p><ul><li>not writing algorithms</li></ul><p>In reality they are just translating a (shitty) language aka &quot;business requirements&quot; into &quot;code&quot;.</p><p>There are a lot of frameworks, like ruby on rails, that do a lot of the work for you (for example a website that enables users to leave their email address).
So you don&#x27;t have to code, you just have to learn some weird syntax.</p><p>Ruby on rails, React (or similar) --&gt; Web app
CRUD apps &lt;-- Create, Read, Update, Delete</p><p>Frontend (View)
Database (Model)
Backend/Business Logic (Controller)</p><p>This whole thing might soon be automated by AI.</p><ol><li>Build a CRUD app contracting firm</li><li>Record all the inputs of my developers (translators for business requirements --&gt; code)</li><li>Train an AI model to translate &quot;business requirements&quot; --&gt; code</li></ol><p>So writing these kind of apps, is nothing like writing binary search algorithms or other lower level stuff, which is taught in school.
These two things are completely separate.</p><h1>Hacking</h1><p>Input --&gt; System --&gt; Output</p><p>To gain access to the System you need to know:</p><ul><li>What input achieves my desired outcome?</li></ul><p>Often times you can give a system an input that it does not expect and thus manage to get access to the system.
Figure out how to make the function behave how you want.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="pure-model">Pure model<a href="#pure-model" class="hash-link" aria-label="Direct link to Pure model" title="Direct link to Pure model">​</a></h2><p>Domain --&gt; Function --&gt; Range
y = f(x)</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="impure-model">Impure model<a href="#impure-model" class="hash-link" aria-label="Direct link to Impure model" title="Direct link to Impure model">​</a></h2><p>Function can output something outside of the range.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="example">Example<a href="#example" class="hash-link" aria-label="Direct link to Example" title="Direct link to Example">​</a></h2><p>Let&#x27;s say we want to cancel a flight without paying the cancellation fee.
We can think of a few of the inputs we have to that system (airline agent can press a button on pc to cancel flight and waive the fee):</p><ul><li>We can call the agent and ask them</li><li>We have a huge amount of words to choose from for that conversation</li><li>We can do it in person</li><li>We can send an email</li><li>Or we can do something unexpected, or out of bounds of the &quot;usual&quot; input, like finding personal information about the agent and blackmailing them (not recommended)</li></ul><p>This way we could essentially hack the system.</p><h1>Guide for Software Engineers</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="high-brow-software-engineering">High Brow Software Engineering<a href="#high-brow-software-engineering" class="hash-link" aria-label="Direct link to High Brow Software Engineering" title="Direct link to High Brow Software Engineering">​</a></h2><ol><li>Understand a complex system</li><li>Modify the system to add a new feature</li><li>Test and ship the new system</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="machine-learning-engineer">Machine Learning Engineer<a href="#machine-learning-engineer" class="hash-link" aria-label="Direct link to Machine Learning Engineer" title="Direct link to Machine Learning Engineer">​</a></h2><ol><li>Download a paper</li><li>Implement the paper</li><li>Keep doing this until you are good</li></ol><h1>Funnels</h1><p>Funnels are essentially just a series of filters that can be applied to some group of things to get to the desired outcome.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="selling-cars">Selling cars<a href="#selling-cars" class="hash-link" aria-label="Direct link to Selling cars" title="Direct link to Selling cars">​</a></h2><ol><li>Top of the funnel: Advertise | 10000 people</li><li>Middle of the funnel: Test drive | 100 people</li><li>Bottom of the funnel: Buy | 5 people</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="getting-a-partner">Getting a partner<a href="#getting-a-partner" class="hash-link" aria-label="Direct link to Getting a partner" title="Direct link to Getting a partner">​</a></h2><ol><li>Send a message | 100 people</li><li>Get a response | 30 people</li><li>Get a date | 5 people</li><li>Lays | 2 person</li><li>Partner | 1 person</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="getting-money">Getting money<a href="#getting-money" class="hash-link" aria-label="Direct link to Getting money" title="Direct link to Getting money">​</a></h2><p>Capitalism.</p><p>Buyers and Sellers.
Both need to consent.</p><p>So we need to convince others to give us money.</p><p>How to get 1.000.000$?</p><ul><li>1$ from 1.000.000 people<ul><li>online only</li></ul></li><li>1.000$ from 1.000 people<ul><li>A couple phone calls can be spent to close deal</li></ul></li><li>1.000.000$ from 1 person<ul><li>A lot of effort can be spent to close deal</li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="million-subs-on-instagram">Million Subs on Instagram<a href="#million-subs-on-instagram" class="hash-link" aria-label="Direct link to Million Subs on Instagram" title="Direct link to Million Subs on Instagram">​</a></h2><p>Followers and Influencers.
Both need to consent.</p><p>Convince 1.000.000 others to follow you.</p><ol><li>Appealing content<ul><li>&quot;Novelty&quot;</li><li>&quot;Shock&quot;</li><li>&quot;Beauty&quot;</li><li>&quot;Sexuality&quot;</li><li>&quot;Comedy&quot;</li></ul></li><li>Be famous<ul><li>FOMO &lt;-- Fear of missing something positive</li><li>&quot;car crash&quot; &lt;-- Fear of missing out on something negative</li></ul></li><li>Dark arts<ul><li>Buy followers<ul><li>Cracked accounts</li><li>New accounts</li></ul></li><li>Make Instagram private<ul><li>Mystery (Whats behind the curtain?)</li></ul></li></ul></li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="wasting-time">Wasting time<a href="#wasting-time" class="hash-link" aria-label="Direct link to Wasting time" title="Direct link to Wasting time">​</a></h2><p>Existentialism --&gt; You make your own meaning.</p><p>Don&#x27;t fall in other people&#x27;s funnels.
Don&#x27;t be in skinner boxes.
Don&#x27;t be influenced by advertising.</p><h1>What to learn</h1><p>Object level skills, like specific frameworks, or languages, are not that important.
They will die out at some point.</p><p>Meta level skills, like how to learn, are more important.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="example-data-science">Example Data Science<a href="#example-data-science" class="hash-link" aria-label="Direct link to Example Data Science" title="Direct link to Example Data Science">​</a></h2><p>Learning statistics is more important than learning Pytorch.
Statistics will always be useful, but Pytorch will be replaced at some point.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="knowledge-tree">Knowledge Tree<a href="#knowledge-tree" class="hash-link" aria-label="Direct link to Knowledge Tree" title="Direct link to Knowledge Tree">​</a></h2><p>Integrate new information into the tree. Build a world model.
In the case of not knowing something you are still able to make decent decisions based on interpolation.</p><p>The root of the tree might be something like physics, such that every data point that gets included in your tree can be distilled down to the smallest particles known in physics at the time.
This enables predictions about unknown data points.</p><p>Another tree root, arguably more advanced than physics, is information.
This is based on the paradigm from before (Input --&gt; System --&gt; Output).
This obviously includes physics as a sub tree.</p><p>&quot;There are 2 hard problems in computer science: cache invalidation, naming things, and off-by-1 errors.&quot;</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="leetcode-interviews">Leetcode Interviews<a href="#leetcode-interviews" class="hash-link" aria-label="Direct link to Leetcode Interviews" title="Direct link to Leetcode Interviews">​</a></h2><p>Insulting</p><p>Just shows if you can grind leetcode not if you are intelligent or a good programmer.
Programming challenges with an objective metric and a slight time limit.</p></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="description" content="Task"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/hri">Human Robot Interaction</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2023-11-07T12:35:59.000Z" itemprop="datePublished">November 7, 2023</time> · <!-- -->18 min read</div></header><div class="markdown" itemprop="articleBody"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="task">Task<a href="#task" class="hash-link" aria-label="Direct link to Task" title="Direct link to Task">​</a></h2><p>HRI interface comparisons with examples (VR, AR, bio-signal-based)</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="comparison-vr-ar-bio-signal-based">Comparison VR, AR, Bio-signal-based<a href="#comparison-vr-ar-bio-signal-based" class="hash-link" aria-label="Direct link to Comparison VR, AR, Bio-signal-based" title="Direct link to Comparison VR, AR, Bio-signal-based">​</a></h2><p>Advantage of all of them is that the user can often interact with the robot in a natural way through hand and body gestures. This makes it possible for users without the technical knowledge of controlling the robot traditionally, to control the robot.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="virtual-realityvr">Virtual reality(VR)<a href="#virtual-realityvr" class="hash-link" aria-label="Direct link to Virtual reality(VR)" title="Direct link to Virtual reality(VR)">​</a></h3><p>Virtual reality puts a human into a virtual world to interact with a robot. The human can see the robot and the robot can see the human. The human can interact with the robot by using a controller or by using their hands.
One important aspect is ability to get almost instant feedback from the robot motion. This is important for the human to be able to learn how to control the robot.
VR headsets can often be uncomfortable to wear for long periods of time. Newer headsets have batteries instead of cable connections, which can be better or worse depending on the use case.
VR could technically do the save as AR does, by just recording the world around the human and displaying parts of it in VR. However the technology isn&#x27;t there yet to perfectly display reality, so there is still clearly a difference.
Could pre-render the actions given to the robot, before executing them.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="augmented-realityar">Augmented reality(AR)<a href="#augmented-realityar" class="hash-link" aria-label="Direct link to Augmented reality(AR)" title="Direct link to Augmented reality(AR)">​</a></h3><p>Augmented reality enhances the real world around the human with digital information to better interact with a robot. The human can see the robot and the robot can see the human. The human can interact with the robot by using a controller or by using their hands.
One difference to VR is the ability to also see and better interact with the real world around the human.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="bio-signal-based">Bio-signal-based<a href="#bio-signal-based" class="hash-link" aria-label="Direct link to Bio-signal-based" title="Direct link to Bio-signal-based">​</a></h3><p>Bio-signal-based devices can be used to control robots. Many different types of bio-signal-based devices exist, such as EEG, EOG, EMG, ECG, ERG, EGG, GSR and EDA.</p><ul><li>Electroencephalography (EEG): Measures electrical activity of the brain.</li><li>Electrooculography (EOG): Measures electrical activity of the eye.</li><li>Electromyography (EMG): Measures electrical activity of the muscles.</li><li>Electrocardiography (ECG): Measures electrical activity of the heart.</li><li>Electroretinography (ERG): Measures electrical activity of the retina.</li><li>Electroglottography (EGG): Measures electrical activity of the vocal cords.</li><li>Galvanic skin response (GSR)/Electrodermal activity (EDA): Measures electrical activity of the skin.</li></ul><p>These devices can be used to control robots in many different ways. For example, a person can control a robot by thinking about moving it, or by moving their eyes to look at different parts of the robot. Bio-signal-based devices can also be used to control robots by measuring the person&#x27;s heart rate, or by measuring the person&#x27;s sweat levels.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="some-abbreviations">Some abbreviations<a href="#some-abbreviations" class="hash-link" aria-label="Direct link to Some abbreviations" title="Direct link to Some abbreviations">​</a></h3><ul><li>ROS: Robot Operating System</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="general-papers">General papers<a href="#general-papers" class="hash-link" aria-label="Direct link to General papers" title="Direct link to General papers">​</a></h3><ul><li><p><a href="https://link.springer.com/content/pdf/10.1007/s43154-020-00005-6.pdf" target="_blank" rel="noopener noreferrer">https://link.springer.com/content/pdf/10.1007/s43154-020-00005-6.pdf</a></p><ul><li>overview of different HRI interfaces</li></ul></li><li><p><a href="https://graphics.cs.wisc.edu/Papers/2017/LRMG17/roman-vr-2017.pdf" target="_blank" rel="noopener noreferrer">https://graphics.cs.wisc.edu/Papers/2017/LRMG17/roman-vr-2017.pdf</a></p><ul><li>general paper about VR as a HRI interface</li></ul></li><li><p><a href="http://ti.rutgers.edu/publications/papers/1999_ieee_tra.pdf" target="_blank" rel="noopener noreferrer">http://ti.rutgers.edu/publications/papers/1999_ieee_tra.pdf</a></p><ul><li>paper about using VR for HRI</li><li>decent overview of VR</li></ul></li><li><p><a href="https://robotics.mit.edu/teleoperating-robots-virtual-reality" target="_blank" rel="noopener noreferrer">https://robotics.mit.edu/teleoperating-robots-virtual-reality</a></p><ul><li>MIT article</li></ul></li><li><p><a href="https://www.allerin.com/blog/ar-vr-and-other-ways-of-controlling-robots" target="_blank" rel="noopener noreferrer">https://www.allerin.com/blog/ar-vr-and-other-ways-of-controlling-robots</a></p><ul><li>article about different HRI interfaces</li><li>might be perfect overview</li></ul></li><li><p><a href="https://www.mdpi.com/1424-8220/21/20/6863" target="_blank" rel="noopener noreferrer">https://www.mdpi.com/1424-8220/21/20/6863</a></p><ul><li>huge summary/survey of bio-signal-based solutions</li><li>for assistance/rehabilitation</li></ul></li><li><p><a href="https://arxiv.org/pdf/2203.03254.pdf" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2203.03254.pdf</a></p><ul><li>AR summary</li><li>2022 paper</li></ul></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="general-comparisons">General comparisons<a href="#general-comparisons" class="hash-link" aria-label="Direct link to General comparisons" title="Direct link to General comparisons">​</a></h3><ul><li><p><a href="https://reader.elsevier.com/reader/sd/pii/S2212827120314815?token=674B622691122E381C72A6FD9A55D0F0163342C7E2F3F3785601BAECC912EB05ED29318E11A2834A7D0B9019B9EE27A6&amp;originRegion=eu-west-1&amp;originCreation=20221104125245" target="_blank" rel="noopener noreferrer">https://reader.elsevier.com/reader/sd/pii/S2212827120314815?token=674B622691122E381C72A6FD9A55D0F0163342C7E2F3F3785601BAECC912EB05ED29318E11A2834A7D0B9019B9EE27A6&amp;originRegion=eu-west-1&amp;originCreation=20221104125245</a></p><ul><li>Review of VR/AR solutions for HRI</li></ul></li><li><p><a href="https://cs.brown.edu/people/er35/publications/SIEDS_2020.pdf" target="_blank" rel="noopener noreferrer">https://cs.brown.edu/people/er35/publications/SIEDS_2020.pdf</a></p><ul><li>comparison of different VR approaches</li><li>positional control (waypoint navigation)</li><li>trajectory control (click and drag)</li></ul></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="get-at-least-one-paper-with-an-example-for-every-interface-type-vr-ar-bio-signal-based">get at least one paper with an example for every interface type (VR, AR, bio-signal-based)<a href="#get-at-least-one-paper-with-an-example-for-every-interface-type-vr-ar-bio-signal-based" class="hash-link" aria-label="Direct link to get at least one paper with an example for every interface type (VR, AR, bio-signal-based)" title="Direct link to get at least one paper with an example for every interface type (VR, AR, bio-signal-based)">​</a></h3><h4 class="anchor anchorWithStickyNavbar_LWe7" id="vr">VR<a href="#vr" class="hash-link" aria-label="Direct link to VR" title="Direct link to VR">​</a></h4><ul><li><a href="https://arxiv.org/pdf/1903.10064.pdf" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/1903.10064.pdf</a><ul><li>controlling a swarm of robots with VR</li><li>manipulating the environment in VR, zooming in and out</li><li>placing walls in the environment to block the robots</li><li>highlights intuitiveness of VR</li><li>gestures are intuitive, but need some training</li><li>visual information from the robots gets sent to pc and dynamically rendered in VR</li><li>technically human swarm interaction (HSI)</li><li>summary:
VR is used in \cite to control a swarm of robots. The robots are able to navigate and interact with each other on their own.
The user can use VR to manipulate the environment, zoom in and out, and place walls in the environment to block or guide the robots. Additionally the robots can be picked up and placed in a new location. Leap Motion is used to identify the users motions.
Thus the user can propose future actions or locations in the virtual environment and the robots will try to execute or move to them in the real world.
The authors conducted a usability study with 10 participants between the ages 20 and 35 with an engineering background. Is showed that the controls are intuitive and the test missions are accelerated with the help of human intervention. They note however that some of the gestures, specifically the wall placement and the world resizing, need some training to get used to.</li></ul></li></ul><ul><li><a href="https://h2r.cs.brown.edu/wp-content/uploads/whitney18.pdf" target="_blank" rel="noopener noreferrer">https://h2r.cs.brown.edu/wp-content/uploads/whitney18.pdf</a></li><li><a href="https://cs.brown.edu/people/gdk/pubs/vr_teleop.pdf" target="_blank" rel="noopener noreferrer">https://cs.brown.edu/people/gdk/pubs/vr_teleop.pdf</a><ul><li>controlling robots over the internet with VR (teleoperation)</li><li>created interface to be used by other researchers</li><li>can be used with consumer-grade headsets</li><li>testing approach: <a href="https://cs.brown.edu/people/er35/publications/testing.pdf" target="_blank" rel="noopener noreferrer">https://cs.brown.edu/people/er35/publications/testing.pdf</a><ul><li>establishes baseline for other research</li></ul></li></ul></li><li><a href="https://arxiv.org/pdf/1703.01270.pdf" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/1703.01270.pdf</a><ul><li>control of robot arms in VR</li><li>VR Control Room</li><li>highlights collocation capabilities of VR</li><li>pick, place, assembly, manufacturing tasks</li><li>summary:
In \cite a team of researchers use VR to control a robot arm. The robot has two arms and is equipped with a camera at its &quot;head&quot;. The user uses the consumer-grade headset Oculus Rift CV1 and two Razer Hydra hand trackers as controllers.
In VR the robot can then be controlled from a control room, which includes the view of the main camera and two optional views from the two robot arms. So the user feels as if they were in the robots head.
To test the system, the authors made an expert user pick up and assemble a Lego piece. They compared it to an automated algorithm on the same task and were able to show that the human performed perfect, whereas the algorithm showed some weakness on the assembly. The user reported that the cameras in the robot arms helped with the alignment of the pieces.
The teleoperation allows the user to perform actions from a save environment.
The paper highlights the ability of VR to utilize consumer-grade hardware.</li></ul></li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="ar">AR<a href="#ar" class="hash-link" aria-label="Direct link to AR" title="Direct link to AR">​</a></h4><ul><li><a href="https://www.frontiersin.org/articles/10.3389/frobt.2017.00020/full" target="_blank" rel="noopener noreferrer">https://www.frontiersin.org/articles/10.3389/frobt.2017.00020/full</a><ul><li>uses tablet</li><li>displays information about the robots motion on the tablet</li><li>one tiltable camera, 1/3 of workspace visible at a time</li><li>uses the tablet to control the robot</li><li>3 interfaces: control with accelerometer of tablet<ul><li>egocentric: user sees the workspace from the robots perspective. Parts of the workspace are not observable due to the lack of field of view and movement of the camera.</li><li>exocentric: user sees the workspace from a fixed position on the ceiling. Vision under the robot arm is blocked, so some objects can&#x27;t be interacted with.</li><li>mobile mixed reality: user sees workspace from tablet in arbitrary position. Can access any location.</li></ul></li><li>pretrial (place one box somewhere else)  was easier with AR plot over workspace enabled</li><li>mobile performs best</li><li>article about it: <a href="https://thenewstack.io/smartphone-app-can-control-robots-augmented-reality/" target="_blank" rel="noopener noreferrer">https://thenewstack.io/smartphone-app-can-control-robots-augmented-reality/</a></li><li>summary:
AR can be used to enhance the environment. In \cite the authors compare 3 interfaces. One egocentric, with a tiltable camera on the robots head, one exocentric, with the camera on the ceiling looking down, and the proposed method of using a mobile tablet as the camera. All three approaches use the tablets accelerometer to control the robots arms. The main advantage of the proposed method is, that its cameras field of view can reach all places, unlike the other two.
The users can see an overlay over the workspace on the tablet screen, where the robots maximum range of motion and potential actions can be projected.
When testing the system, users performed better on the pretrial, when having the AR plot enabled. Additionally the mobile reality interface shows better performance than the other two.
The main points to take away, are that this approach needs visual markers in the environment, the user and the robot need to be in the same environment for the mobile version and the AR overlay helps the user and the robot interact better.</li></ul></li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="bio-signal-based-1">Bio-signal-based<a href="#bio-signal-based-1" class="hash-link" aria-label="Direct link to Bio-signal-based" title="Direct link to Bio-signal-based">​</a></h4><ul><li><p><a href="https://link.springer.com/article/10.1007/s10514-020-09916-x" target="_blank" rel="noopener noreferrer">https://link.springer.com/article/10.1007/s10514-020-09916-x</a></p><ul><li>earlier work used only EEG: <a href="http://groups.csail.mit.edu/drl/wiki/images/e/ec/Correcting_Robot_Mistakes_in_Real_Time_Using_EEG_Signals.pdf" target="_blank" rel="noopener noreferrer">http://groups.csail.mit.edu/drl/wiki/images/e/ec/Correcting_Robot_Mistakes_in_Real_Time_Using_EEG_Signals.pdf</a></li><li>Uses EMG(muscle) + EEG(brain) to give swift feedback to robot</li><li>EMG is used to detect the users intention, EEG is used to detect potential errors the robot or the human makes</li><li>summary:
In the paper \cite the authors used a hybrid of electromyography (EMG) and electroencephalography (EEG) to control a arm with a tool on it. The robot was supposed to hit one of three holes in the wall in front of it with the electric screwdriver in their hand. The user is equipped with electrodes on their head and surface bar electrodes are applied to their forearms. The signals of those devices are processed separately and then used to determine the action of the robot arm.
The user observes the robot and its environment directly and tries to move the tool in the robots hand via muscle movements. When the robot or the user themself make a mistake, the users brain reacts a certain way, often unconsciously, which can be detected by the EEG processor. Those signals are then used to stop and then correct the robot.
The system was evaluated on 7 participants. The users were untrained, to reduce the hurdle for new users. The correct target was hit in roughly 70% of the trials, when the robot randomly chose. With the help of the correction through the participant, the success rate jumped to 97%.
The authors concluded, that the reliability needs to be improved to be able to deploy the system in safety critical situations. Specifically, the neural network that classified the EEG signal into mistake or no-mistake, had only a 54% accuracy. They also highlight that more users would be needed to make the system more robust towards inter-person variations. However, the system shows potential for an effective supervision system.</li></ul></li><li><p><a href="https://www.jmir.org/2019/10/e16194/" target="_blank" rel="noopener noreferrer">https://www.jmir.org/2019/10/e16194/</a></p><ul><li>neuralink whitepaper</li><li>uses brain signals to control a robot</li><li>might be interesting, but not used on humans yet</li><li>don&#x27;t know if it &quot;counts&quot; as an example</li><li>mainly describes a way to get information out of the human brain, not however how to interpret it or control a robot.</li><li>but really important</li></ul></li><li><p><a href="https://static.aminer.org/pdf/PDF/000/329/658/emg_based_human_machine_interface_system.pdf" target="_blank" rel="noopener noreferrer">https://static.aminer.org/pdf/PDF/000/329/658/emg_based_human_machine_interface_system.pdf</a></p><ul><li>example of using EMG to control a robot</li><li>really old paper</li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="comparison">Comparison<a href="#comparison" class="hash-link" aria-label="Direct link to Comparison" title="Direct link to Comparison">​</a></h2><p>AR is the cheapest of the three, as no special hardware is needed most of the time. VR however has huge upside of remote operation, by emerging the user in the distant environment. Additionally VR can be more intuitive because the user can be &quot;in the skin&quot; of the robot. Bio-signal-based solutions are in the early stages but offer huge potential for swift intuitive interaction with robots.</p><table><thead><tr><th></th><th>Example use cases</th><th>Example devices</th><th>ease of use</th><th>unique functions</th><th>cost</th><th>future potential</th></tr></thead><tbody><tr><td>VR</td><td>control robot motion over internet by moving controllers and observing results</td><td>Oculus rift, Meta quest pro, smartphone</td><td>special equipment necessary (headset and controllers), often uncomfortable for long periods of time, either battery (limited work time) or cables (limited motion range)</td><td>teleoperation, see whole environment of the robot from somewhere else; ego perspective and feel of robot (step into skin of robot, more hands on), but strong stable internet connection necessary</td><td>expensive special equipment, getting cheaper when consumer grade equipment can be used</td><td>might become important to remotely help out &quot;almost fully&quot; autonomous systems in difficult situations; need better form factors</td></tr><tr><td>AR</td><td>display important robot information about the robot(range of motion, wear and tear, pre-rendering of action)</td><td>google glasses, tablet, smartphone</td><td>really simple</td><td>no special equipment required</td><td>pretty low, no special equipment</td><td>integration into normal glasses, or contact lenses</td></tr><tr><td>Bio-signal-based</td><td>signal if robot did right or wrong action directly with ones mind, control of prosthesis via muscle signals(EMG)</td><td>implants (Neuralink), EEG, EMG, etc.</td><td>some special equipment needed, sometimes easy to use (wrist bands), sometimes permanent augment (implant)</td><td>if implemented well, can read the humans mind and make robot smooth extension of human</td><td>ranges from cheap(wrist bands) to expensive(implants)</td><td>huge potential to merge with robots and full control of a robot with a humans thoughts</td></tr></tbody></table><h3 class="anchor anchorWithStickyNavbar_LWe7" id="use-cases">Use cases<a href="#use-cases" class="hash-link" aria-label="Direct link to Use cases" title="Direct link to Use cases">​</a></h3><ul><li><p>VR</p><ul><li>teleoperation</li><li>swarm operation</li><li>full birds eye view or different perspective</li></ul></li><li><p>AR</p><ul><li>display important information about the environment and the robot</li></ul></li><li><p>Bio-signal-based</p><ul><li>control of robot</li><li>possibly more complex, and faster controls possible</li></ul></li><li><p>Comparison
VR has the special property that it can transport the user into a completely different environment to control a robot through teleoperation. Additionally one can view the environment from any perspective, for example a birds eye view, as in \ref. This can help to gain an overview over the environment and thus control swarms or other robots.
AR and bio-signal-based technologies have direct visual contact from the user or through the camera of a handheld device, like a tablet \ref, most of the time.
However, AR can enhance the real environment with important information about the workspace and the robot. This can help the user to perform the tasks faster and saver. It is to be noted that technically VR can do the same, by recording the environment with its front camera and displaying the information in the headset, but the user might have a lower field of view compared to AR glasses or a tablet.
Bio-signal-based technologies can be used to control the robot directly with ones mind (EEG) or muscles (EMG), like in \ref. The applications are still limited to simple controls of robot arms or the detection of mistakes with the human mind.
The main difference to AR and VR is the fact that the reactions can be faster as the thinking about the mistake can be detected unconsciously by the system. The main issue is that the reliability is still low and thus not save to use with big and powerful robots.</p></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="devices">Devices<a href="#devices" class="hash-link" aria-label="Direct link to Devices" title="Direct link to Devices">​</a></h3><ul><li><p>VR</p><ul><li>Meta quest 2</li><li>smartphone</li></ul></li><li><p>AR</p><ul><li>google glasses</li><li>tablet</li><li>smartphone</li></ul></li><li><p>Bio-signal-based</p><ul><li>EEG</li><li>EMG</li><li>implants (Neuralink)</li></ul></li><li><p>Comparison
VR devices are mostly headsets to display the environment with controllers to control the robot and the position of the user. For headsets, the Meta Quest 2/Pro or the Valve Index can be used. For the controllers, Razer Hydra hand trackers or the default VR controllers that come with the headsets are available. The user can also use a smartphone as a headset, but the field of view is limited, the performance might not be enough and the resolution is not as good as with a dedicated headset.
For AR, dedicated glasses are still early in the development. However handheld devices like tablets or smartphones can be used as well, as in \ref.
Bio-signal-based devices can be wrist bands, that measure muscle contraction, electrodes on the scalp to measure signals from the brain or various other specialized technology. One main difference is that VR and AR devices are bought on the consumer market, which can help with cost and development, whereas bio-signal-based devices aren&#x27;t often used in everyday live.</p></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="ease-of-use">Ease of use<a href="#ease-of-use" class="hash-link" aria-label="Direct link to Ease of use" title="Direct link to Ease of use">​</a></h3><ul><li><p>VR</p><ul><li>special equipment necessary (headset and controllers), often uncomfortable for long periods of time, either battery (limited work time) or cables (limited motion range)</li><li>intuitive, ego perspective</li></ul></li><li><p>AR</p><ul><li>really simple</li><li>need to control by touchscreen, which is not as intuitive as VR</li></ul></li><li><p>Bio-signal-based</p><ul><li>some special equipment needed, sometimes easy to use (wrist bands), sometimes permanent augment (implant)</li></ul></li><li><p>comparison</p></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="cost">Cost<a href="#cost" class="hash-link" aria-label="Direct link to Cost" title="Direct link to Cost">​</a></h3><p>Table:</p><table><thead><tr><th>technology</th><th>device</th><th>cost</th><th>link</th></tr></thead><tbody><tr><td>VR</td><td>Meta Quest 2</td><td>450$</td><td><a href="https://www.meta.com/de/en/quest/products/quest-2/" target="_blank" rel="noopener noreferrer">https://www.meta.com/de/en/quest/products/quest-2/</a></td></tr><tr><td>VR</td><td>Valve Index</td><td>1079$</td><td><a href="https://store.steampowered.com/valveindex" target="_blank" rel="noopener noreferrer">https://store.steampowered.com/valveindex</a></td></tr><tr><td>AR</td><td>I-pad</td><td>449$</td><td><a href="https://www.apple.com/shop/buy-ipad/ipad" target="_blank" rel="noopener noreferrer">https://www.apple.com/shop/buy-ipad/ipad</a></td></tr><tr><td>AR</td><td>Galaxy Tab S8</td><td>200$</td><td><a href="https://www.samsung.com/us/tablets/galaxy-tab-s8/buy/" target="_blank" rel="noopener noreferrer">https://www.samsung.com/us/tablets/galaxy-tab-s8/buy/</a></td></tr><tr><td>AR</td><td>Google Glasses</td><td>999$</td><td><a href="https://www.theverge.com/2019/5/20/18632689/google-glass-enterprise-edition-2-augmented-reality-headset-pricing" target="_blank" rel="noopener noreferrer">https://www.theverge.com/2019/5/20/18632689/google-glass-enterprise-edition-2-augmented-reality-headset-pricing</a></td></tr><tr><td>Bio-signal-based</td><td>EEG electrode hat</td><td>1500$</td><td><a href="https://shop.openbci.com/collections/frontpage" target="_blank" rel="noopener noreferrer">https://shop.openbci.com/collections/frontpage</a></td></tr></tbody></table><ul><li>comparison
To compare the cost of the different technologies, The prices of the different devices were looked up and summarized in \ref. Note that this is only a fraction of possible devices.
The low end Meta Quest 2 in the same price range as the high end I-Pad. But when comparing the more powerful Valve Index, to a more budget tablet, like the Galaxy Tab S8, VR devices are considerably more expensive than a basic AR device. Additionally for most VR headsets, an additional high end PC is necessary to process the visuals.
Another alternative for AR are the Google Glasses, which come at a higher price, similar to the VR headsets.
Bio-signal-based devices, specifically EEG, are starting at the price of a VR headset. They might however get cheaper if those devices get produced in higher numbers. The prices can get way higher as well, if implants through operations are used.
So in general, AR is the cheapest option, as one can simply use a smartphone or a tablet. VR requires some special technology in form of a headset and probably a high end PC. Finally, the bio-signal-based devices come out as most expensive, as they are still early in development.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="problems">Problems<a href="#problems" class="hash-link" aria-label="Direct link to Problems" title="Direct link to Problems">​</a></h3><p>The main ways VR and AR can improve from today are general hardware improvements like better batteries, </p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="future-potential">Future potential<a href="#future-potential" class="hash-link" aria-label="Direct link to Future potential" title="Direct link to Future potential">​</a></h3><ul><li><p>VR</p><ul><li>might become important to remotely help out &quot;almost fully&quot; autonomous systems in difficult situations</li><li>need better form factors and better hardware:<ul><li>batteries</li><li>more comfortable</li></ul></li></ul></li><li><p>AR</p><ul><li>integration into normal glasses, or contact lenses</li><li>more powerful hardware, or remote processing</li></ul></li><li><p>Bio-signal-based</p><ul><li>huge potential to merge with robots and full control of a robot with a humans thoughts</li><li>more consumer based hardware</li><li>improved reliability</li></ul></li><li><p>Comparison:
VR might be used at some point to have the human help out almost fully autonomous systems by stepping in the perspective of the robot. Or it can be used to fully control robots remotely and remove the need for humans to work in dangerous environments.
AR could have a huge jump in usability if it were to be integrated into everyday glasses or even contact lenses. This could enable people without training to use them. If robots are more common in everyday life this might increase the trust in the robots by displaying certain information about the robots future actions in the environment.
Bio-signal-based technologies could be used to completely and reliably control robots with human thoughts, which would be a huge step in the field of human-robot interaction. If this technology is achieved, most other control devices might be obsolete.
So the biggest potential certainly lies within EEG technologies, as they can enable a direct link between human and robot. However the other two technologies might also play a crucial role in some more niche cases.</p></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2><ul><li><p>Summarize the key points and findings of the paper:
In summary, it is difficult to compare the three technologies, because they each have their different use cases, as seen in \ref. Additionally, they are never tested against each other, with regard to user feedback.
When comparing the use cases, VR shows a clear advantage in teleoperation, AR in merging digital information into the real world environment and bio-signal-based technology can use quick reactions directly from the human brain to mitigate mistakes.</p></li><li><p>Highlight the main contributions of the paper and its impact on the field of HRI interfaces:
This paper compares some examples of the three technologies and their use cases. It also extrapolates those comparisons to the whole categories. Hopefully it can give some ideas on the future research directions of the field. Additionally, this is an encouragement to further investigate how to better compare the three technologies to then be able to better predict what technology is worth more efforts. To conclude this report, some recommendations for future research are the following.</p></li><li><p>Discuss future directions for research in HRI interfaces, including VR, AR, and bio-signal-based:</p></li></ul><p>The final achievement would be to have a direct link between human and robot in both directions. Until then, all three technologies will need to be improved gradually.
For VR, the ability to wear the headset for a long time and training programs should be the focus.
AR might be more useful, if the technology gets integrated better into glasses to not need an extra tablet while working with a robot in the workspace.
Bio-signal-based technologies first need to improve their reliability before they can be used in real-world applications. A next step would be to improve the designs behind the devices, so they can be used more for consumer products and accelerate the development.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="todo">todo<a href="#todo" class="hash-link" aria-label="Direct link to todo" title="Direct link to todo">​</a></h2><ul><li>add picture maybe</li><li>add VR/AR review</li></ul></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="description" content="Some wiki stuff:"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/logic">Logic</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2023-11-07T12:35:59.000Z" itemprop="datePublished">November 7, 2023</time> · <!-- -->One min read</div></header><div class="markdown" itemprop="articleBody"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="some-wiki-stuff">Some wiki stuff:<a href="#some-wiki-stuff" class="hash-link" aria-label="Direct link to Some wiki stuff:" title="Direct link to Some wiki stuff:">​</a></h2><ul><li><a href="https://en.wikipedia.org/wiki/Argument_map" target="_blank" rel="noopener noreferrer">Argument map</a></li><li><a href="https://en.wikipedia.org/wiki/Logical_form" target="_blank" rel="noopener noreferrer">Logical form</a></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="idea">Idea<a href="#idea" class="hash-link" aria-label="Direct link to Idea" title="Direct link to Idea">​</a></h2><p>It would be cool to have a somewhat standarised form of arguments or moral systems. This could be in form of an Argument map in a huge tree and implemented on a website or in a program.</p><p>So one could create ones own tree of axioms, premisses and conclusions. Those could be shared and argued about. The program could help identifiy contentions between two peoples moral systems so one can instantly focus on the disagreements.</p><p>The main issue is the uglieness of human input. Even with a some standard blocks like axioms and other logical forms humans still input their claims differently. So one would need some machine learning to interpret and compare arguments. With the advancements of language models like <a href="https://en.wikipedia.org/wiki/GPT-3" target="_blank" rel="noopener noreferrer">GTP-3</a> one may be able to achieve some decent results.</p></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="description" content="Preparation"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/machine_learning">Machine Learning</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2023-11-07T12:35:59.000Z" itemprop="datePublished">November 7, 2023</time> · <!-- -->2 min read</div></header><div class="markdown" itemprop="articleBody"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="preparation">Preparation<a href="#preparation" class="hash-link" aria-label="Direct link to Preparation" title="Direct link to Preparation">​</a></h2><p>Going Through <a href="https://cs50.harvard.edu/college/2022/spring/notes/0/" target="_blank" rel="noopener noreferrer">CS50</a> for refresh of some basics (<a href="/blog/cs50">Notes</a>).</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="sources">Sources<a href="#sources" class="hash-link" aria-label="Direct link to Sources" title="Direct link to Sources">​</a></h2><p><a href="https://machinelearningmastery.com/start-here/" target="_blank" rel="noopener noreferrer">Roadmap/Plan</a><br>
<a href="http://karpathy.github.io/2022/03/14/lecun1989/" target="_blank" rel="noopener noreferrer">Motivation/Karpathy is a cool dude</a></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="problem-description">Problem description<a href="#problem-description" class="hash-link" aria-label="Direct link to Problem description" title="Direct link to Problem description">​</a></h2><blockquote><p>Find a model or procedure that makes best use of historical data comprised of inputs and outputs in order to skillfully predict outputs given new and unseen inputs in the future. <a href="https://machinelearningmastery.com/think-machine-learning/#:~:text=Find%20a%20model%20or%20procedure%20that%20makes%20best%20use%20of%20historical%20data%20comprised%20of%20inputs%20and%20outputs%20in%20order%20to%20skillfully%20predict%20outputs%20given%20new%20and%20unseen%20inputs%20in%20the%20future." target="_blank" rel="noopener noreferrer">[1]</a></p></blockquote><h2 class="anchor anchorWithStickyNavbar_LWe7" id="problem-solution">Problem solution<a href="#problem-solution" class="hash-link" aria-label="Direct link to Problem solution" title="Direct link to Problem solution">​</a></h2><blockquote><p>A model or procedure that automatically creates the most likely approximation of the unknown underlying relationship between inputs and associated outputs in historical data. <a href="https://machinelearningmastery.com/think-machine-learning/#:~:text=as%20the%20following%3A-,A%20model%20or%20procedure%20that%20automatically%20creates%20the%20most%20likely%20approximation%20of%20the%20unknown%20underlying%20relationship%20between%20inputs%20and%20associated%20outputs%20in%20historical%20data.,-Again%2C%20this%20is" target="_blank" rel="noopener noreferrer">[1]</a></p></blockquote><h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-get-there">How to get there<a href="#how-to-get-there" class="hash-link" aria-label="Direct link to How to get there" title="Direct link to How to get there">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="define-the-problem">Define the problem<a href="#define-the-problem" class="hash-link" aria-label="Direct link to Define the problem" title="Direct link to Define the problem">​</a></h3><ul><li>Describe problem informally and formally and list assumptions and similar problems</li><li>List motivations for solving the problem, the benefits a solution provides and how the solution will be used.</li><li>Describe how the problem could be solved manually.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="prepare-data">Prepare Data<a href="#prepare-data" class="hash-link" aria-label="Direct link to Prepare Data" title="Direct link to Prepare Data">​</a></h3><ul><li>Consider what data is available, what data is missing and what data can be removed.</li><li>Organize your selected data by formatting, cleaning and sampling from it.</li><li>Transform preprocessed data into features ready for machine learning.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="spot-check-algorithms">Spot check algorithms<a href="#spot-check-algorithms" class="hash-link" aria-label="Direct link to Spot check algorithms" title="Direct link to Spot check algorithms">​</a></h3><ul><li>create small experiment with different transformations of the dataset and different standard algorithms</li><li>run every pair a bunch of times and compare mean and variance</li><li>helps flushing out the problem structure and getting the algorithms on which to focus in the next steps</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="improving-results">Improving Results<a href="#improving-results" class="hash-link" aria-label="Direct link to Improving Results" title="Direct link to Improving Results">​</a></h3><ul><li>Search through parameter space to find best performing models</li><li>Ensemble: combine results of multiple models</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="present-results">Present Results<a href="#present-results" class="hash-link" aria-label="Direct link to Present Results" title="Direct link to Present Results">​</a></h3><ul><li>Define the context of the problem and the motivation</li><li>Describe Problem as a question that got answered</li><li>Concisely describe the solution as an answer to the question</li><li>Specify limitations of the model, what questions it can&#x27;t answer and with what probability it can answer questions</li></ul></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="description" content="Intuitive understanding"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/neural_network">Neural network</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2023-11-07T12:35:59.000Z" itemprop="datePublished">November 7, 2023</time> · <!-- -->One min read</div></header><div class="markdown" itemprop="articleBody"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="intuitive-understanding">Intuitive understanding<a href="#intuitive-understanding" class="hash-link" aria-label="Direct link to Intuitive understanding" title="Direct link to Intuitive understanding">​</a></h2><p>A neural network is pretty much just a function that maps a bunch of inputs to a bunch of outputs. First that function does bad at mapping. By showing a lot of input/output pairs the parameters in the function get adjusted to improve the mapping.</p><p>So there are three big parts of a neural network. The architecture of the network, the optimization of the parameters and the amount and quality of the data.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="architecture">Architecture<a href="#architecture" class="hash-link" aria-label="Direct link to Architecture" title="Direct link to Architecture">​</a></h2><ul><li>How many layers?</li><li>What type of layers?</li><li>What activation functions?</li><li>Input and output dimensions?</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="optimization">Optimization<a href="#optimization" class="hash-link" aria-label="Direct link to Optimization" title="Direct link to Optimization">​</a></h2><ul><li>What does the loss function look like?</li><li>Gradient descent?</li><li>What optimizer?</li><li>When and how fast to change the parameters?</li><li>When to stop training?</li><li>Is there overfitting?</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="data">Data<a href="#data" class="hash-link" aria-label="Direct link to Data" title="Direct link to Data">​</a></h2><ul><li>How much data is there?</li><li>Is Data argumentation necessary and/or useful?</li><li>Can there be too much data?</li><li>Is there bias in data?</li></ul><h1>Practical Stuff</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="perceptron">Perceptron<a href="#perceptron" class="hash-link" aria-label="Direct link to Perceptron" title="Direct link to Perceptron">​</a></h2><p>The Perceptron is the simplest neural network possible.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="implement-small-deep-learning-library-from-scratch-with-numpy">Implement small deep learning library from scratch (with numpy)<a href="#implement-small-deep-learning-library-from-scratch-with-numpy" class="hash-link" aria-label="Direct link to Implement small deep learning library from scratch (with numpy)" title="Direct link to Implement small deep learning library from scratch (with numpy)">​</a></h2><p>At some point!! To help with a deeper understanding of backpropagation and the inner workings in general.</p></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="description" content="Destiny notes"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/notes">notes</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2023-11-07T12:35:59.000Z" itemprop="datePublished">November 7, 2023</time> · <!-- -->One min read</div></header><div class="markdown" itemprop="articleBody"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="destiny-notes">Destiny notes<a href="#destiny-notes" class="hash-link" aria-label="Direct link to Destiny notes" title="Direct link to Destiny notes">​</a></h2><ul><li>Website to organize Destiny&#x27;s arguments in a nice format with logic structure</li><li>link to segments/proofs</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="overarching-points">overarching points<a href="#overarching-points" class="hash-link" aria-label="Direct link to overarching points" title="Direct link to overarching points">​</a></h3><ul><li>All the points are supposed to be examples of the application of a system</li><li>This system is used to generate good outcomes in your live</li><li>So don&#x27;t try to copy the outcome of the points, as they are based on Destiny&#x27;s subjective values and environment</li><li>Instead try to understand the system and apply it to your own life</li></ul><h3></h3><ul><li>Act as a sounding board when talking with emotional friend</li><li>if you have the choice between burning someone to the ground or leaving it neutral, leave it neutral</li><li>be really careful when comparing people, especially when both are friends<ul><li>e.g. Comparing body parts</li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="ai-vs-human">AI vs human<a href="#ai-vs-human" class="hash-link" aria-label="Direct link to AI vs human" title="Direct link to AI vs human">​</a></h2><ul><li>Website with question</li><li>need to select real answer, from real and AI generated answer</li><li>AI generated answer is instructed to sound like human (gpt3 api)</li></ul></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="description" content="Data Loading"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/pytorch">PyTorch</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2023-11-07T12:35:59.000Z" itemprop="datePublished">November 7, 2023</time> · <!-- -->One min read</div></header><div class="markdown" itemprop="articleBody"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="data-loading">Data Loading<a href="#data-loading" class="hash-link" aria-label="Direct link to Data Loading" title="Direct link to Data Loading">​</a></h2><p>For a custom dataset one needs to implement the Dataset class even if its the most basic dataset.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="derivatives">Derivatives<a href="#derivatives" class="hash-link" aria-label="Direct link to Derivatives" title="Direct link to Derivatives">​</a></h2><p><a href="https://machinelearningmastery.com/calculating-derivatives-in-pytorch/" target="_blank" rel="noopener noreferrer"></a></p></div></article><nav class="pagination-nav" aria-label="Blog list page navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/blog"><div class="pagination-nav__label">Newer Entries</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/blog/page/3"><div class="pagination-nav__label">Older Entries</div></a></nav></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Socials</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://www.github.com/mokronos" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.linkedin.com/in/sebastian-hirt-574862278/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Linkedin<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2023 | Sebastian Hirt | Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.3350fcfb.js"></script>
<script src="/assets/js/main.204eeccb.js"></script>
</body>
</html>