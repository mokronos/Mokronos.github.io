<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.3">
<title data-rh="true">Human Robot Interaction | Mokronos&#x27;s Site</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://mokronos.github.io/blog/hri"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="Human Robot Interaction | Mokronos&#x27;s Site"><meta data-rh="true" name="description" content="Task"><meta data-rh="true" property="og:description" content="Task"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2023-10-30T13:16:27.000Z"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://mokronos.github.io/blog/hri"><link data-rh="true" rel="alternate" href="https://mokronos.github.io/blog/hri" hreflang="en"><link data-rh="true" rel="alternate" href="https://mokronos.github.io/blog/hri" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Mokronos&#39;s Site RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Mokronos&#39;s Site Atom Feed">




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous">
<script src="js/hotkey.js"></script><link rel="stylesheet" href="/assets/css/styles.12cf0569.css">
<link rel="preload" href="/assets/js/runtime~main.26ffc47b.js" as="script">
<link rel="preload" href="/assets/js/main.5fea4c6f.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"dark")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="My Site Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/logo.svg" alt="My Site Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">Mokronos&#x27;s Site</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/mokronos" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently dark mode)" aria-label="Switch between dark and light mode (currently dark mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search"><span aria-label="expand searchbar" role="button" class="search-icon" tabindex="0"></span><input type="search" id="search_input_react" placeholder="Loading..." aria-label="Search" class="navbar__search-input search-bar" disabled=""></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/ai">Artificial Intelligence</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/bayesian_thinking">Bayesian thinking</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/classes">classes</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/coding">Coding</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/cs50">CS50</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/daily">Daily ToDos</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/data_preparation">Data Preparation and Feature Engineering</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/deep_learning">Training Guide</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/docker">Docker</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/electric_vehicles">Electric vehicles</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/etf_investing">Investing</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/fakenews">Fake news</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/habits">Habits</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/how_to_code">How to code</a></li><li class="sidebarItem__DBe"><a aria-current="page" class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href="/blog/hri">Human Robot Interaction</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/logic">Logic</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/machine_learning">Machine Learning</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/neural_network">Neural network</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/notes">notes</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/pytorch">PyTorch</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/search">Search</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/sort">Sort</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/tidy_data">Tidy data</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/unknown">Knowing the unknown</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><article itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="description" content="Task"><header><h1 class="title_f1Hy" itemprop="headline">Human Robot Interaction</h1><div class="container_mt6G margin-vert--md"><time datetime="2023-10-30T13:16:27.000Z" itemprop="datePublished">October 30, 2023</time> · <!-- -->18 min read</div></header><div id="__blog-post-container" class="markdown" itemprop="articleBody"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="task">Task<a href="#task" class="hash-link" aria-label="Direct link to Task" title="Direct link to Task">​</a></h2><p>HRI interface comparisons with examples (VR, AR, bio-signal-based)</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="comparison-vr-ar-bio-signal-based">Comparison VR, AR, Bio-signal-based<a href="#comparison-vr-ar-bio-signal-based" class="hash-link" aria-label="Direct link to Comparison VR, AR, Bio-signal-based" title="Direct link to Comparison VR, AR, Bio-signal-based">​</a></h2><p>Advantage of all of them is that the user can often interact with the robot in a natural way through hand and body gestures. This makes it possible for users without the technical knowledge of controlling the robot traditionally, to control the robot.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="virtual-realityvr">Virtual reality(VR)<a href="#virtual-realityvr" class="hash-link" aria-label="Direct link to Virtual reality(VR)" title="Direct link to Virtual reality(VR)">​</a></h3><p>Virtual reality puts a human into a virtual world to interact with a robot. The human can see the robot and the robot can see the human. The human can interact with the robot by using a controller or by using their hands.
One important aspect is ability to get almost instant feedback from the robot motion. This is important for the human to be able to learn how to control the robot.
VR headsets can often be uncomfortable to wear for long periods of time. Newer headsets have batteries instead of cable connections, which can be better or worse depending on the use case.
VR could technically do the save as AR does, by just recording the world around the human and displaying parts of it in VR. However the technology isn&#x27;t there yet to perfectly display reality, so there is still clearly a difference.
Could pre-render the actions given to the robot, before executing them.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="augmented-realityar">Augmented reality(AR)<a href="#augmented-realityar" class="hash-link" aria-label="Direct link to Augmented reality(AR)" title="Direct link to Augmented reality(AR)">​</a></h3><p>Augmented reality enhances the real world around the human with digital information to better interact with a robot. The human can see the robot and the robot can see the human. The human can interact with the robot by using a controller or by using their hands.
One difference to VR is the ability to also see and better interact with the real world around the human.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="bio-signal-based">Bio-signal-based<a href="#bio-signal-based" class="hash-link" aria-label="Direct link to Bio-signal-based" title="Direct link to Bio-signal-based">​</a></h3><p>Bio-signal-based devices can be used to control robots. Many different types of bio-signal-based devices exist, such as EEG, EOG, EMG, ECG, ERG, EGG, GSR and EDA.</p><ul><li>Electroencephalography (EEG): Measures electrical activity of the brain.</li><li>Electrooculography (EOG): Measures electrical activity of the eye.</li><li>Electromyography (EMG): Measures electrical activity of the muscles.</li><li>Electrocardiography (ECG): Measures electrical activity of the heart.</li><li>Electroretinography (ERG): Measures electrical activity of the retina.</li><li>Electroglottography (EGG): Measures electrical activity of the vocal cords.</li><li>Galvanic skin response (GSR)/Electrodermal activity (EDA): Measures electrical activity of the skin.</li></ul><p>These devices can be used to control robots in many different ways. For example, a person can control a robot by thinking about moving it, or by moving their eyes to look at different parts of the robot. Bio-signal-based devices can also be used to control robots by measuring the person&#x27;s heart rate, or by measuring the person&#x27;s sweat levels.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="some-abbreviations">Some abbreviations<a href="#some-abbreviations" class="hash-link" aria-label="Direct link to Some abbreviations" title="Direct link to Some abbreviations">​</a></h3><ul><li>ROS: Robot Operating System</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="general-papers">General papers<a href="#general-papers" class="hash-link" aria-label="Direct link to General papers" title="Direct link to General papers">​</a></h3><ul><li><p><a href="https://link.springer.com/content/pdf/10.1007/s43154-020-00005-6.pdf" target="_blank" rel="noopener noreferrer">https://link.springer.com/content/pdf/10.1007/s43154-020-00005-6.pdf</a></p><ul><li>overview of different HRI interfaces</li></ul></li><li><p><a href="https://graphics.cs.wisc.edu/Papers/2017/LRMG17/roman-vr-2017.pdf" target="_blank" rel="noopener noreferrer">https://graphics.cs.wisc.edu/Papers/2017/LRMG17/roman-vr-2017.pdf</a></p><ul><li>general paper about VR as a HRI interface</li></ul></li><li><p><a href="http://ti.rutgers.edu/publications/papers/1999_ieee_tra.pdf" target="_blank" rel="noopener noreferrer">http://ti.rutgers.edu/publications/papers/1999_ieee_tra.pdf</a></p><ul><li>paper about using VR for HRI</li><li>decent overview of VR</li></ul></li><li><p><a href="https://robotics.mit.edu/teleoperating-robots-virtual-reality" target="_blank" rel="noopener noreferrer">https://robotics.mit.edu/teleoperating-robots-virtual-reality</a></p><ul><li>MIT article</li></ul></li><li><p><a href="https://www.allerin.com/blog/ar-vr-and-other-ways-of-controlling-robots" target="_blank" rel="noopener noreferrer">https://www.allerin.com/blog/ar-vr-and-other-ways-of-controlling-robots</a></p><ul><li>article about different HRI interfaces</li><li>might be perfect overview</li></ul></li><li><p><a href="https://www.mdpi.com/1424-8220/21/20/6863" target="_blank" rel="noopener noreferrer">https://www.mdpi.com/1424-8220/21/20/6863</a></p><ul><li>huge summary/survey of bio-signal-based solutions</li><li>for assistance/rehabilitation</li></ul></li><li><p><a href="https://arxiv.org/pdf/2203.03254.pdf" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2203.03254.pdf</a></p><ul><li>AR summary</li><li>2022 paper</li></ul></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="general-comparisons">General comparisons<a href="#general-comparisons" class="hash-link" aria-label="Direct link to General comparisons" title="Direct link to General comparisons">​</a></h3><ul><li><p><a href="https://reader.elsevier.com/reader/sd/pii/S2212827120314815?token=674B622691122E381C72A6FD9A55D0F0163342C7E2F3F3785601BAECC912EB05ED29318E11A2834A7D0B9019B9EE27A6&amp;originRegion=eu-west-1&amp;originCreation=20221104125245" target="_blank" rel="noopener noreferrer">https://reader.elsevier.com/reader/sd/pii/S2212827120314815?token=674B622691122E381C72A6FD9A55D0F0163342C7E2F3F3785601BAECC912EB05ED29318E11A2834A7D0B9019B9EE27A6&amp;originRegion=eu-west-1&amp;originCreation=20221104125245</a></p><ul><li>Review of VR/AR solutions for HRI</li></ul></li><li><p><a href="https://cs.brown.edu/people/er35/publications/SIEDS_2020.pdf" target="_blank" rel="noopener noreferrer">https://cs.brown.edu/people/er35/publications/SIEDS_2020.pdf</a></p><ul><li>comparison of different VR approaches</li><li>positional control (waypoint navigation)</li><li>trajectory control (click and drag)</li></ul></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="get-at-least-one-paper-with-an-example-for-every-interface-type-vr-ar-bio-signal-based">get at least one paper with an example for every interface type (VR, AR, bio-signal-based)<a href="#get-at-least-one-paper-with-an-example-for-every-interface-type-vr-ar-bio-signal-based" class="hash-link" aria-label="Direct link to get at least one paper with an example for every interface type (VR, AR, bio-signal-based)" title="Direct link to get at least one paper with an example for every interface type (VR, AR, bio-signal-based)">​</a></h3><h4 class="anchor anchorWithStickyNavbar_LWe7" id="vr">VR<a href="#vr" class="hash-link" aria-label="Direct link to VR" title="Direct link to VR">​</a></h4><ul><li><a href="https://arxiv.org/pdf/1903.10064.pdf" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/1903.10064.pdf</a><ul><li>controlling a swarm of robots with VR</li><li>manipulating the environment in VR, zooming in and out</li><li>placing walls in the environment to block the robots</li><li>highlights intuitiveness of VR</li><li>gestures are intuitive, but need some training</li><li>visual information from the robots gets sent to pc and dynamically rendered in VR</li><li>technically human swarm interaction (HSI)</li><li>summary:
VR is used in \cite to control a swarm of robots. The robots are able to navigate and interact with each other on their own.
The user can use VR to manipulate the environment, zoom in and out, and place walls in the environment to block or guide the robots. Additionally the robots can be picked up and placed in a new location. Leap Motion is used to identify the users motions.
Thus the user can propose future actions or locations in the virtual environment and the robots will try to execute or move to them in the real world.
The authors conducted a usability study with 10 participants between the ages 20 and 35 with an engineering background. Is showed that the controls are intuitive and the test missions are accelerated with the help of human intervention. They note however that some of the gestures, specifically the wall placement and the world resizing, need some training to get used to.</li></ul></li></ul><ul><li><a href="https://h2r.cs.brown.edu/wp-content/uploads/whitney18.pdf" target="_blank" rel="noopener noreferrer">https://h2r.cs.brown.edu/wp-content/uploads/whitney18.pdf</a></li><li><a href="https://cs.brown.edu/people/gdk/pubs/vr_teleop.pdf" target="_blank" rel="noopener noreferrer">https://cs.brown.edu/people/gdk/pubs/vr_teleop.pdf</a><ul><li>controlling robots over the internet with VR (teleoperation)</li><li>created interface to be used by other researchers</li><li>can be used with consumer-grade headsets</li><li>testing approach: <a href="https://cs.brown.edu/people/er35/publications/testing.pdf" target="_blank" rel="noopener noreferrer">https://cs.brown.edu/people/er35/publications/testing.pdf</a><ul><li>establishes baseline for other research</li></ul></li></ul></li><li><a href="https://arxiv.org/pdf/1703.01270.pdf" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/1703.01270.pdf</a><ul><li>control of robot arms in VR</li><li>VR Control Room</li><li>highlights collocation capabilities of VR</li><li>pick, place, assembly, manufacturing tasks</li><li>summary:
In \cite a team of researchers use VR to control a robot arm. The robot has two arms and is equipped with a camera at its &quot;head&quot;. The user uses the consumer-grade headset Oculus Rift CV1 and two Razer Hydra hand trackers as controllers.
In VR the robot can then be controlled from a control room, which includes the view of the main camera and two optional views from the two robot arms. So the user feels as if they were in the robots head.
To test the system, the authors made an expert user pick up and assemble a Lego piece. They compared it to an automated algorithm on the same task and were able to show that the human performed perfect, whereas the algorithm showed some weakness on the assembly. The user reported that the cameras in the robot arms helped with the alignment of the pieces.
The teleoperation allows the user to perform actions from a save environment.
The paper highlights the ability of VR to utilize consumer-grade hardware.</li></ul></li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="ar">AR<a href="#ar" class="hash-link" aria-label="Direct link to AR" title="Direct link to AR">​</a></h4><ul><li><a href="https://www.frontiersin.org/articles/10.3389/frobt.2017.00020/full" target="_blank" rel="noopener noreferrer">https://www.frontiersin.org/articles/10.3389/frobt.2017.00020/full</a><ul><li>uses tablet</li><li>displays information about the robots motion on the tablet</li><li>one tiltable camera, 1/3 of workspace visible at a time</li><li>uses the tablet to control the robot</li><li>3 interfaces: control with accelerometer of tablet<ul><li>egocentric: user sees the workspace from the robots perspective. Parts of the workspace are not observable due to the lack of field of view and movement of the camera.</li><li>exocentric: user sees the workspace from a fixed position on the ceiling. Vision under the robot arm is blocked, so some objects can&#x27;t be interacted with.</li><li>mobile mixed reality: user sees workspace from tablet in arbitrary position. Can access any location.</li></ul></li><li>pretrial (place one box somewhere else)  was easier with AR plot over workspace enabled</li><li>mobile performs best</li><li>article about it: <a href="https://thenewstack.io/smartphone-app-can-control-robots-augmented-reality/" target="_blank" rel="noopener noreferrer">https://thenewstack.io/smartphone-app-can-control-robots-augmented-reality/</a></li><li>summary:
AR can be used to enhance the environment. In \cite the authors compare 3 interfaces. One egocentric, with a tiltable camera on the robots head, one exocentric, with the camera on the ceiling looking down, and the proposed method of using a mobile tablet as the camera. All three approaches use the tablets accelerometer to control the robots arms. The main advantage of the proposed method is, that its cameras field of view can reach all places, unlike the other two.
The users can see an overlay over the workspace on the tablet screen, where the robots maximum range of motion and potential actions can be projected.
When testing the system, users performed better on the pretrial, when having the AR plot enabled. Additionally the mobile reality interface shows better performance than the other two.
The main points to take away, are that this approach needs visual markers in the environment, the user and the robot need to be in the same environment for the mobile version and the AR overlay helps the user and the robot interact better.</li></ul></li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="bio-signal-based-1">Bio-signal-based<a href="#bio-signal-based-1" class="hash-link" aria-label="Direct link to Bio-signal-based" title="Direct link to Bio-signal-based">​</a></h4><ul><li><p><a href="https://link.springer.com/article/10.1007/s10514-020-09916-x" target="_blank" rel="noopener noreferrer">https://link.springer.com/article/10.1007/s10514-020-09916-x</a></p><ul><li>earlier work used only EEG: <a href="http://groups.csail.mit.edu/drl/wiki/images/e/ec/Correcting_Robot_Mistakes_in_Real_Time_Using_EEG_Signals.pdf" target="_blank" rel="noopener noreferrer">http://groups.csail.mit.edu/drl/wiki/images/e/ec/Correcting_Robot_Mistakes_in_Real_Time_Using_EEG_Signals.pdf</a></li><li>Uses EMG(muscle) + EEG(brain) to give swift feedback to robot</li><li>EMG is used to detect the users intention, EEG is used to detect potential errors the robot or the human makes</li><li>summary:
In the paper \cite the authors used a hybrid of electromyography (EMG) and electroencephalography (EEG) to control a arm with a tool on it. The robot was supposed to hit one of three holes in the wall in front of it with the electric screwdriver in their hand. The user is equipped with electrodes on their head and surface bar electrodes are applied to their forearms. The signals of those devices are processed separately and then used to determine the action of the robot arm.
The user observes the robot and its environment directly and tries to move the tool in the robots hand via muscle movements. When the robot or the user themself make a mistake, the users brain reacts a certain way, often unconsciously, which can be detected by the EEG processor. Those signals are then used to stop and then correct the robot.
The system was evaluated on 7 participants. The users were untrained, to reduce the hurdle for new users. The correct target was hit in roughly 70% of the trials, when the robot randomly chose. With the help of the correction through the participant, the success rate jumped to 97%.
The authors concluded, that the reliability needs to be improved to be able to deploy the system in safety critical situations. Specifically, the neural network that classified the EEG signal into mistake or no-mistake, had only a 54% accuracy. They also highlight that more users would be needed to make the system more robust towards inter-person variations. However, the system shows potential for an effective supervision system.</li></ul></li><li><p><a href="https://www.jmir.org/2019/10/e16194/" target="_blank" rel="noopener noreferrer">https://www.jmir.org/2019/10/e16194/</a></p><ul><li>neuralink whitepaper</li><li>uses brain signals to control a robot</li><li>might be interesting, but not used on humans yet</li><li>don&#x27;t know if it &quot;counts&quot; as an example</li><li>mainly describes a way to get information out of the human brain, not however how to interpret it or control a robot.</li><li>but really important</li></ul></li><li><p><a href="https://static.aminer.org/pdf/PDF/000/329/658/emg_based_human_machine_interface_system.pdf" target="_blank" rel="noopener noreferrer">https://static.aminer.org/pdf/PDF/000/329/658/emg_based_human_machine_interface_system.pdf</a></p><ul><li>example of using EMG to control a robot</li><li>really old paper</li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="comparison">Comparison<a href="#comparison" class="hash-link" aria-label="Direct link to Comparison" title="Direct link to Comparison">​</a></h2><p>AR is the cheapest of the three, as no special hardware is needed most of the time. VR however has huge upside of remote operation, by emerging the user in the distant environment. Additionally VR can be more intuitive because the user can be &quot;in the skin&quot; of the robot. Bio-signal-based solutions are in the early stages but offer huge potential for swift intuitive interaction with robots.</p><table><thead><tr><th></th><th>Example use cases</th><th>Example devices</th><th>ease of use</th><th>unique functions</th><th>cost</th><th>future potential</th></tr></thead><tbody><tr><td>VR</td><td>control robot motion over internet by moving controllers and observing results</td><td>Oculus rift, Meta quest pro, smartphone</td><td>special equipment necessary (headset and controllers), often uncomfortable for long periods of time, either battery (limited work time) or cables (limited motion range)</td><td>teleoperation, see whole environment of the robot from somewhere else; ego perspective and feel of robot (step into skin of robot, more hands on), but strong stable internet connection necessary</td><td>expensive special equipment, getting cheaper when consumer grade equipment can be used</td><td>might become important to remotely help out &quot;almost fully&quot; autonomous systems in difficult situations; need better form factors</td></tr><tr><td>AR</td><td>display important robot information about the robot(range of motion, wear and tear, pre-rendering of action)</td><td>google glasses, tablet, smartphone</td><td>really simple</td><td>no special equipment required</td><td>pretty low, no special equipment</td><td>integration into normal glasses, or contact lenses</td></tr><tr><td>Bio-signal-based</td><td>signal if robot did right or wrong action directly with ones mind, control of prosthesis via muscle signals(EMG)</td><td>implants (Neuralink), EEG, EMG, etc.</td><td>some special equipment needed, sometimes easy to use (wrist bands), sometimes permanent augment (implant)</td><td>if implemented well, can read the humans mind and make robot smooth extension of human</td><td>ranges from cheap(wrist bands) to expensive(implants)</td><td>huge potential to merge with robots and full control of a robot with a humans thoughts</td></tr></tbody></table><h3 class="anchor anchorWithStickyNavbar_LWe7" id="use-cases">Use cases<a href="#use-cases" class="hash-link" aria-label="Direct link to Use cases" title="Direct link to Use cases">​</a></h3><ul><li><p>VR</p><ul><li>teleoperation</li><li>swarm operation</li><li>full birds eye view or different perspective</li></ul></li><li><p>AR</p><ul><li>display important information about the environment and the robot</li></ul></li><li><p>Bio-signal-based</p><ul><li>control of robot</li><li>possibly more complex, and faster controls possible</li></ul></li><li><p>Comparison
VR has the special property that it can transport the user into a completely different environment to control a robot through teleoperation. Additionally one can view the environment from any perspective, for example a birds eye view, as in \ref. This can help to gain an overview over the environment and thus control swarms or other robots.
AR and bio-signal-based technologies have direct visual contact from the user or through the camera of a handheld device, like a tablet \ref, most of the time.
However, AR can enhance the real environment with important information about the workspace and the robot. This can help the user to perform the tasks faster and saver. It is to be noted that technically VR can do the same, by recording the environment with its front camera and displaying the information in the headset, but the user might have a lower field of view compared to AR glasses or a tablet.
Bio-signal-based technologies can be used to control the robot directly with ones mind (EEG) or muscles (EMG), like in \ref. The applications are still limited to simple controls of robot arms or the detection of mistakes with the human mind.
The main difference to AR and VR is the fact that the reactions can be faster as the thinking about the mistake can be detected unconsciously by the system. The main issue is that the reliability is still low and thus not save to use with big and powerful robots.</p></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="devices">Devices<a href="#devices" class="hash-link" aria-label="Direct link to Devices" title="Direct link to Devices">​</a></h3><ul><li><p>VR</p><ul><li>Meta quest 2</li><li>smartphone</li></ul></li><li><p>AR</p><ul><li>google glasses</li><li>tablet</li><li>smartphone</li></ul></li><li><p>Bio-signal-based</p><ul><li>EEG</li><li>EMG</li><li>implants (Neuralink)</li></ul></li><li><p>Comparison
VR devices are mostly headsets to display the environment with controllers to control the robot and the position of the user. For headsets, the Meta Quest 2/Pro or the Valve Index can be used. For the controllers, Razer Hydra hand trackers or the default VR controllers that come with the headsets are available. The user can also use a smartphone as a headset, but the field of view is limited, the performance might not be enough and the resolution is not as good as with a dedicated headset.
For AR, dedicated glasses are still early in the development. However handheld devices like tablets or smartphones can be used as well, as in \ref.
Bio-signal-based devices can be wrist bands, that measure muscle contraction, electrodes on the scalp to measure signals from the brain or various other specialized technology. One main difference is that VR and AR devices are bought on the consumer market, which can help with cost and development, whereas bio-signal-based devices aren&#x27;t often used in everyday live.</p></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="ease-of-use">Ease of use<a href="#ease-of-use" class="hash-link" aria-label="Direct link to Ease of use" title="Direct link to Ease of use">​</a></h3><ul><li><p>VR</p><ul><li>special equipment necessary (headset and controllers), often uncomfortable for long periods of time, either battery (limited work time) or cables (limited motion range)</li><li>intuitive, ego perspective</li></ul></li><li><p>AR</p><ul><li>really simple</li><li>need to control by touchscreen, which is not as intuitive as VR</li></ul></li><li><p>Bio-signal-based</p><ul><li>some special equipment needed, sometimes easy to use (wrist bands), sometimes permanent augment (implant)</li></ul></li><li><p>comparison</p></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="cost">Cost<a href="#cost" class="hash-link" aria-label="Direct link to Cost" title="Direct link to Cost">​</a></h3><p>Table:</p><table><thead><tr><th>technology</th><th>device</th><th>cost</th><th>link</th></tr></thead><tbody><tr><td>VR</td><td>Meta Quest 2</td><td>450$</td><td><a href="https://www.meta.com/de/en/quest/products/quest-2/" target="_blank" rel="noopener noreferrer">https://www.meta.com/de/en/quest/products/quest-2/</a></td></tr><tr><td>VR</td><td>Valve Index</td><td>1079$</td><td><a href="https://store.steampowered.com/valveindex" target="_blank" rel="noopener noreferrer">https://store.steampowered.com/valveindex</a></td></tr><tr><td>AR</td><td>I-pad</td><td>449$</td><td><a href="https://www.apple.com/shop/buy-ipad/ipad" target="_blank" rel="noopener noreferrer">https://www.apple.com/shop/buy-ipad/ipad</a></td></tr><tr><td>AR</td><td>Galaxy Tab S8</td><td>200$</td><td><a href="https://www.samsung.com/us/tablets/galaxy-tab-s8/buy/" target="_blank" rel="noopener noreferrer">https://www.samsung.com/us/tablets/galaxy-tab-s8/buy/</a></td></tr><tr><td>AR</td><td>Google Glasses</td><td>999$</td><td><a href="https://www.theverge.com/2019/5/20/18632689/google-glass-enterprise-edition-2-augmented-reality-headset-pricing" target="_blank" rel="noopener noreferrer">https://www.theverge.com/2019/5/20/18632689/google-glass-enterprise-edition-2-augmented-reality-headset-pricing</a></td></tr><tr><td>Bio-signal-based</td><td>EEG electrode hat</td><td>1500$</td><td><a href="https://shop.openbci.com/collections/frontpage" target="_blank" rel="noopener noreferrer">https://shop.openbci.com/collections/frontpage</a></td></tr></tbody></table><ul><li>comparison
To compare the cost of the different technologies, The prices of the different devices were looked up and summarized in \ref. Note that this is only a fraction of possible devices.
The low end Meta Quest 2 in the same price range as the high end I-Pad. But when comparing the more powerful Valve Index, to a more budget tablet, like the Galaxy Tab S8, VR devices are considerably more expensive than a basic AR device. Additionally for most VR headsets, an additional high end PC is necessary to process the visuals.
Another alternative for AR are the Google Glasses, which come at a higher price, similar to the VR headsets.
Bio-signal-based devices, specifically EEG, are starting at the price of a VR headset. They might however get cheaper if those devices get produced in higher numbers. The prices can get way higher as well, if implants through operations are used.
So in general, AR is the cheapest option, as one can simply use a smartphone or a tablet. VR requires some special technology in form of a headset and probably a high end PC. Finally, the bio-signal-based devices come out as most expensive, as they are still early in development.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="problems">Problems<a href="#problems" class="hash-link" aria-label="Direct link to Problems" title="Direct link to Problems">​</a></h3><p>The main ways VR and AR can improve from today are general hardware improvements like better batteries, </p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="future-potential">Future potential<a href="#future-potential" class="hash-link" aria-label="Direct link to Future potential" title="Direct link to Future potential">​</a></h3><ul><li><p>VR</p><ul><li>might become important to remotely help out &quot;almost fully&quot; autonomous systems in difficult situations</li><li>need better form factors and better hardware:<ul><li>batteries</li><li>more comfortable</li></ul></li></ul></li><li><p>AR</p><ul><li>integration into normal glasses, or contact lenses</li><li>more powerful hardware, or remote processing</li></ul></li><li><p>Bio-signal-based</p><ul><li>huge potential to merge with robots and full control of a robot with a humans thoughts</li><li>more consumer based hardware</li><li>improved reliability</li></ul></li><li><p>Comparison:
VR might be used at some point to have the human help out almost fully autonomous systems by stepping in the perspective of the robot. Or it can be used to fully control robots remotely and remove the need for humans to work in dangerous environments.
AR could have a huge jump in usability if it were to be integrated into everyday glasses or even contact lenses. This could enable people without training to use them. If robots are more common in everyday life this might increase the trust in the robots by displaying certain information about the robots future actions in the environment.
Bio-signal-based technologies could be used to completely and reliably control robots with human thoughts, which would be a huge step in the field of human-robot interaction. If this technology is achieved, most other control devices might be obsolete.
So the biggest potential certainly lies within EEG technologies, as they can enable a direct link between human and robot. However the other two technologies might also play a crucial role in some more niche cases.</p></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2><ul><li><p>Summarize the key points and findings of the paper:
In summary, it is difficult to compare the three technologies, because they each have their different use cases, as seen in \ref. Additionally, they are never tested against each other, with regard to user feedback.
When comparing the use cases, VR shows a clear advantage in teleoperation, AR in merging digital information into the real world environment and bio-signal-based technology can use quick reactions directly from the human brain to mitigate mistakes.</p></li><li><p>Highlight the main contributions of the paper and its impact on the field of HRI interfaces:
This paper compares some examples of the three technologies and their use cases. It also extrapolates those comparisons to the whole categories. Hopefully it can give some ideas on the future research directions of the field. Additionally, this is an encouragement to further investigate how to better compare the three technologies to then be able to better predict what technology is worth more efforts. To conclude this report, some recommendations for future research are the following.</p></li><li><p>Discuss future directions for research in HRI interfaces, including VR, AR, and bio-signal-based:</p></li></ul><p>The final achievement would be to have a direct link between human and robot in both directions. Until then, all three technologies will need to be improved gradually.
For VR, the ability to wear the headset for a long time and training programs should be the focus.
AR might be more useful, if the technology gets integrated better into glasses to not need an extra tablet while working with a robot in the workspace.
Bio-signal-based technologies first need to improve their reliability before they can be used in real-world applications. A next step would be to improve the designs behind the devices, so they can be used more for consumer products and accelerate the development.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="todo">todo<a href="#todo" class="hash-link" aria-label="Direct link to todo" title="Direct link to todo">​</a></h2><ul><li>add picture maybe</li><li>add VR/AR review</li></ul></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/blog/how_to_code"><div class="pagination-nav__sublabel">Newer Post</div><div class="pagination-nav__label">How to code</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/blog/logic"><div class="pagination-nav__sublabel">Older Post</div><div class="pagination-nav__label">Logic</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#task" class="table-of-contents__link toc-highlight">Task</a></li><li><a href="#comparison-vr-ar-bio-signal-based" class="table-of-contents__link toc-highlight">Comparison VR, AR, Bio-signal-based</a><ul><li><a href="#virtual-realityvr" class="table-of-contents__link toc-highlight">Virtual reality(VR)</a></li><li><a href="#augmented-realityar" class="table-of-contents__link toc-highlight">Augmented reality(AR)</a></li><li><a href="#bio-signal-based" class="table-of-contents__link toc-highlight">Bio-signal-based</a></li><li><a href="#some-abbreviations" class="table-of-contents__link toc-highlight">Some abbreviations</a></li><li><a href="#general-papers" class="table-of-contents__link toc-highlight">General papers</a></li><li><a href="#general-comparisons" class="table-of-contents__link toc-highlight">General comparisons</a></li><li><a href="#get-at-least-one-paper-with-an-example-for-every-interface-type-vr-ar-bio-signal-based" class="table-of-contents__link toc-highlight">get at least one paper with an example for every interface type (VR, AR, bio-signal-based)</a></li></ul></li><li><a href="#comparison" class="table-of-contents__link toc-highlight">Comparison</a><ul><li><a href="#use-cases" class="table-of-contents__link toc-highlight">Use cases</a></li><li><a href="#devices" class="table-of-contents__link toc-highlight">Devices</a></li><li><a href="#ease-of-use" class="table-of-contents__link toc-highlight">Ease of use</a></li><li><a href="#cost" class="table-of-contents__link toc-highlight">Cost</a></li><li><a href="#problems" class="table-of-contents__link toc-highlight">Problems</a></li><li><a href="#future-potential" class="table-of-contents__link toc-highlight">Future potential</a></li></ul></li><li><a href="#conclusion" class="table-of-contents__link toc-highlight">Conclusion</a></li><li><a href="#todo" class="table-of-contents__link toc-highlight">todo</a></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Socials</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://www.github.com/mokronos" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.linkedin.com/in/sebastian-hirt-574862278/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Linkedin<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2023 | Sebastian Hirt | Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.26ffc47b.js"></script>
<script src="/assets/js/main.5fea4c6f.js"></script>
</body>
</html>